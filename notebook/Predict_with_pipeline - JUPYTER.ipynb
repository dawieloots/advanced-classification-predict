{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "executionInfo": {
     "elapsed": 722,
     "status": "ok",
     "timestamp": 1712658469641,
     "user": {
      "displayName": "Dawie Loots",
      "userId": "00798578190767538914"
     },
     "user_tz": -120
    },
    "id": "h20rlqi05cIw",
    "outputId": "3fe254f3-df89-4b99-f0f6-3f19fad51df0",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dawie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dawie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Libraries for data loading, data manipulation and data visulisation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import chardet # To provide a best estimate of the encoding that was used in the text data\n",
    "import io # For string operations\n",
    "%matplotlib inline\n",
    "\n",
    "# Libraries for data preparation and model building\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer, TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import datetime\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import math\n",
    "import re\n",
    "from sklearn.utils import resample\n",
    "from scipy.sparse import vstack\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import feature_selection\n",
    "from sklearn.feature_selection import SelectPercentile, SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score, log_loss\n",
    "from sklearn.model_selection import GridSearchCV, KFold, StratifiedKFold\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE,SMOTENC\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3843,
     "status": "ok",
     "timestamp": 1712662249775,
     "user": {
      "displayName": "Dawie Loots",
      "userId": "00798578190767538914"
     },
     "user_tz": -120
    },
    "id": "2hMYSMSB6fHp",
    "outputId": "21e3d6e6-a716-471f-ddf8-ff44c47b82ec"
   },
   "outputs": [],
   "source": [
    "# LOAD DATASET\n",
    "\n",
    "def class_distribution(data):\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        unique_classes, class_counts = data.iloc[:, 0].value_counts().index, data.iloc[:, 0].value_counts().values\n",
    "    elif isinstance(data, pd.Series):\n",
    "        unique_classes, class_counts = data.value_counts().index, data.value_counts().values\n",
    "    class_dict = {}\n",
    "    for class_name, count in zip(unique_classes, class_counts):\n",
    "        class_perc = round(count/len(data),3)\n",
    "        class_dict.update({class_name: class_perc})\n",
    "    return class_dict\n",
    "\n",
    "def load_dataset(environment, size):\n",
    "    if environment == 'colab':\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        csv_file = '/content/drive/MyDrive/Professionele ontwikkeling/Data Science/Explore Data Science Course/Sprint 6_Advanced Classification/Predict/advanced-classification-predict/data/train.csv'\n",
    "    else:\n",
    "        csv_file = r'G:\\My Drive\\Professionele ontwikkeling\\Data Science\\Explore Data Science Course\\Sprint 6_Advanced Classification\\Predict\\advanced-classification-predict\\data\\train.csv'\n",
    "    df = pd.read_csv(csv_file)\n",
    "    print(f'Dataset original shape: {df.shape}')\n",
    "    print(f'Dataset original class distribution: {class_distribution(df)}')\n",
    "    sample_size = int(len(df) * size)\n",
    "    if size == 1:\n",
    "        pass\n",
    "    else:\n",
    "        X = df.drop(columns=['sentiment']).copy()\n",
    "        y = df.sentiment.copy()\n",
    "        X_sample, _, y_sample, _ = train_test_split(X, y, train_size=sample_size, stratify=y, random_state=42)\n",
    "        df = pd.concat([X_sample, y_sample], axis=1)\n",
    "\n",
    "#        np.random.seed(42)  # OLD CODE\n",
    "#       pd.read_csv(csv_file).shape[0] - n  # OLD CODE\n",
    "#        df = pd.read_csv(csv_file, skiprows=lambda i: i > 0 and np.random.rand() > n / (i + 1), nrows=n)  # OLD CODE\n",
    "    return df, sample_size\n",
    "\n",
    "\n",
    "# def load_full_dataset():  # OLD CODE\n",
    "#   df = pd.read_csv(csv_file)  # OLD CODE\n",
    "#   return df, 'full'  # OLD CODE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUVu-k0BXiQ3"
   },
   "source": [
    "df_train = pd.read_csv('G:/MyDrive/Professionele ontwikkeling/Data Science/Explore Data Science Course/Sprint 6_Advanced Classification/Predict/advanced-classification-predict/data/train.csv')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1712658473396,
     "user": {
      "displayName": "Dawie Loots",
      "userId": "00798578190767538914"
     },
     "user_tz": -120
    },
    "id": "1rI10jOolRbZ"
   },
   "outputs": [],
   "source": [
    "class NoiseRemover(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        pattern_url = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'   # Find all hyperlinks\n",
    "        subs_url = r''\n",
    "        X_transformed = pd.Series(X).replace(to_replace = pattern_url, value = subs_url, regex = True)\n",
    "        return X_transformed\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1712658473396,
     "user": {
      "displayName": "Dawie Loots",
      "userId": "00798578190767538914"
     },
     "user_tz": -120
    },
    "id": "npvvupgdqXbC"
   },
   "outputs": [],
   "source": [
    "class EmoticonConverter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        emoticon_dictionary = {':\\)': 'smiley_face_emoticon',\n",
    "                               ':\\(': 'frowning_face_emoticon',\n",
    "                               ':D': 'grinning_face_emoticon',\n",
    "                               ':P': 'sticking_out_tongue_emoticon',\n",
    "                               ';\\)': 'winking_face_emoticon',\n",
    "                               ':o': 'surprised_face_emoticon',\n",
    "                               ':\\|': 'neutral_face_emoticon',\n",
    "                               ':\\'\\)': 'tears_of_joy_emoticon',\n",
    "                               ':\\'\\(': 'crying_face_emoticon'}\n",
    "        X_transformed = X.replace(emoticon_dictionary, regex=True)\n",
    "        return X_transformed\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 610,
     "status": "ok",
     "timestamp": 1712662354260,
     "user": {
      "displayName": "Dawie Loots",
      "userId": "00798578190767538914"
     },
     "user_tz": -120
    },
    "id": "VTRod0QUsOL7"
   },
   "outputs": [],
   "source": [
    "class PunctuationRemover(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        def expand_contractions(text):\n",
    "            contractions = {\"'t\": \" not\",\"'s\": \" is\",\"'re\": \" are\",\"'ll\": \" will\", \"'m\": \" am\"}\n",
    "            pattern = re.compile(r\"\\b(\" + \"|\".join(re.escape(key) for key in contractions.keys()) + r\")\\b\")\n",
    "            text = re.sub(r\"n't\\b\", \" not\", text) # Replace \"n't\" with \" not\"\n",
    "            text = pattern.sub(lambda match: contractions[match.group(0)], text) # Replace all other contractions except for \"n't\"\n",
    "            return text\n",
    "\n",
    "        def remove_punctuation(text):\n",
    "            return ''.join([l for l in text if l not in string.punctuation])\n",
    "\n",
    "        X_transformed = X.apply(lambda x: expand_contractions(x))\n",
    "        X_transformed = X_transformed.apply(lambda x: remove_punctuation(x))\n",
    "        return X_transformed\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1712658473397,
     "user": {
      "displayName": "Dawie Loots",
      "userId": "00798578190767538914"
     },
     "user_tz": -120
    },
    "id": "b7cme7kUuyIt"
   },
   "outputs": [],
   "source": [
    "class Tokenizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, type='TweetTokenizer'):\n",
    "        self.type = type\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if self.type == 'TweetTokenizer':\n",
    "            tokenizer = TweetTokenizer()\n",
    "        else:\n",
    "            tokenizer = TreebankWordTokenizer()\n",
    "        X_transformed = X.apply(tokenizer.tokenize)\n",
    "        return X_transformed\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1712658473397,
     "user": {
      "displayName": "Dawie Loots",
      "userId": "00798578190767538914"
     },
     "user_tz": -120
    },
    "id": "-puz8STKxpt3"
   },
   "outputs": [],
   "source": [
    "class StopwordRemover(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        # Remove stopwords using a vectorized operation\n",
    "        X_transformed = X.apply(lambda tokens: [t for t in tokens if t.lower() not in stop_words])\n",
    "        return X_transformed\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1712658473397,
     "user": {
      "displayName": "Dawie Loots",
      "userId": "00798578190767538914"
     },
     "user_tz": -120
    },
    "id": "1miNSgYK4zfz"
   },
   "outputs": [],
   "source": [
    "class Lemmatizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, pos='v'):\n",
    "        self.pos = pos\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        X_transformed = X.apply(lambda tokens: [lemmatizer.lemmatize(word, pos=self.pos) for word in tokens])\n",
    "        return X_transformed\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomSampler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 666,
     "status": "ok",
     "timestamp": 1712662309479,
     "user": {
      "displayName": "Dawie Loots",
      "userId": "00798578190767538914"
     },
     "user_tz": -120
    },
    "id": "XYw7dmak74hm"
   },
   "outputs": [],
   "source": [
    "class Vectorize(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, type='tfidf', max_df=1, min_df=1, ngram_range=(1,1), max_features=None):\n",
    "        self.type = type\n",
    "        self.max_df = max_df\n",
    "        self.min_df = min_df\n",
    "        self.ngram_range = ngram_range\n",
    "        self.max_features = max_features\n",
    "\n",
    "        if self.type == 'count':\n",
    "            self.vectorizer = CountVectorizer(max_features=self.max_features, lowercase=True,\n",
    "                                              max_df=self.max_df, min_df=self.min_df,\n",
    "                                              ngram_range=self.ngram_range)\n",
    "        elif self.type == 'tfidf':\n",
    "            self.vectorizer = TfidfVectorizer(max_features=self.max_features, lowercase=True,\n",
    "                                              max_df=self.max_df, min_df=self.min_df,\n",
    "                                              ngram_range=self.ngram_range)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid vectorizer type. Choose either 'count' or 'tfidf'\")\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X_joined = [' '.join(tokens) for tokens in X]\n",
    "        self.vectorizer.fit(X_joined)\n",
    "        return self.vectorizer.fit(X_joined)\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_joined = [' '.join(tokens) for tokens in X]\n",
    "        return self.vectorizer.transform(X_joined)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1712658473398,
     "user": {
      "displayName": "Dawie Loots",
      "userId": "00798578190767538914"
     },
     "user_tz": -120
    },
    "id": "TP0o7OpfGpg3"
   },
   "outputs": [],
   "source": [
    "class Scaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, type='robust'):\n",
    "        self.type = type\n",
    "\n",
    "        if self.type == 'robust':\n",
    "            self.scaler = RobustScaler(with_centering=False)\n",
    "        elif self.type == 'minmax':\n",
    "            self.scaler = MinMaxScaler(with_centering=False)\n",
    "        elif self.type == 'maxabs':\n",
    "            self.scaler = MaxAbsScaler(with_centering=False)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid scaler type. Choose between 'robust', 'minmax' or 'maxabs'.\")\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.scaler.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return self.scaler.transform(X)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 424,
     "status": "ok",
     "timestamp": 1712662295841,
     "user": {
      "displayName": "Dawie Loots",
      "userId": "00798578190767538914"
     },
     "user_tz": -120
    },
    "id": "R5UyERuHxKKI"
   },
   "outputs": [],
   "source": [
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, type='anova_f', percentile=50):\n",
    "        self.type = type\n",
    "        self.percentile = percentile\n",
    "\n",
    "        if self.type == 'mutualinfo':\n",
    "            self.selector = SelectPercentile(score_func=mutual_info_classif, percentile=percentile)\n",
    "        elif self.type == 'anova_f':\n",
    "            self.selector = SelectPercentile(score_func=f_classif, percentile=percentile)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid selector type. Choose between 'mutualinfo' or 'anova_f'.\")\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.selector.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return self.selector.transform(X)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.selector.fit(X, y)\n",
    "        return self.selector.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_categorical_labels(data):\n",
    "    # Dictionary mapping numerical categories to labels\n",
    "    label_map = {\n",
    "        2: 'News',\n",
    "        1: 'Pro',\n",
    "        0: 'Neutral',\n",
    "        -1: 'Anti'\n",
    "    }\n",
    "    transformed_data = data.map(label_map)\n",
    "    return transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 481,
     "status": "ok",
     "timestamp": 1712664075087,
     "user": {
      "displayName": "Dawie Loots",
      "userId": "00798578190767538914"
     },
     "user_tz": -120
    },
    "id": "5fVqF62W5XKf",
    "outputId": "4744b5d5-2fe8-43fc-b923-f9832c8c44ac"
   },
   "outputs": [],
   "source": [
    "# RUN GRIDSEARCH ON ONE SPECIFIC MODEL\n",
    "\n",
    "df_train, size = load_dataset('jupyter',0.1)\n",
    "df_train['sentiment'] = transform_categorical_labels(df_train['sentiment'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train['message'], df_train['sentiment'], test_size=0.2, random_state=42, stratify=df_train['sentiment'])\n",
    "\n",
    "print(f'Shape of X_train: {X_train.shape}')\n",
    "print(f'Shape of y_train: {y_train.shape}')\n",
    "print(f'Shape of X_test: {X_test.shape}')\n",
    "print(f'Shape of y_test: {y_test.shape}')\n",
    "print(f'Distribution of y_train: {class_distribution(y_train)}')\n",
    "print(f'Distribution of y_test: {class_distribution(y_test)}')\n",
    "\n",
    "# Defining preprocessing steps\n",
    "preprocessing_steps = [\n",
    "                       ('noise_removal', NoiseRemover()),\n",
    "                       ('emoticon_convertion', EmoticonConverter()),\n",
    "                       ('punctuation_removal', PunctuationRemover()),\n",
    "                       ('tokenization', Tokenizer(type='TweetTokenizer')),\n",
    "                       ('stopword_removal', StopwordRemover()),\n",
    "                       ('lemmatization', Lemmatizer()),\n",
    "                       ('vectorization', Vectorize(max_features=2000, type='tfidf')),\n",
    "                       ('smote', SMOTE()),\n",
    "                       ('scaler', Scaler(type='robust')),\n",
    "                       ('feature_selection', FeatureSelector(percentile=50, type='mutualinfo'))\n",
    "                       ]\n",
    "\n",
    "# Create the pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "model = MLPClassifier()\n",
    "model_name = 'MLP'\n",
    "pipeline = Pipeline(preprocessing_steps  + [('model', model)])\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {'tokenization__type': ['TweetTokenizer'],#'TreebankWordTokenizer'],\n",
    "              'vectorization__type': ['tfidf'],\n",
    "              'vectorization__max_df': [0.75],\n",
    "              'vectorization__min_df': [1],#10],\n",
    "              'vectorization__ngram_range': [(1,1)],\n",
    "              'vectorization__max_features': [None],\n",
    "              'scaler__type': ['robust'],#'minmax','maxabs'],\n",
    "              'feature_selection__type': ['anova_f'],\n",
    "              'feature_selection__percentile': [60],\n",
    "\n",
    "            # Hyperparameters for the model\n",
    "\n",
    "              #'model__n_neighbors': [3],                               #KNN\n",
    "              #'model__weights': [ 'distance'],#'uniform',              #KNN\n",
    "              #'model__metric': ['euclidean']#, 'manhattan'],           #KNN\n",
    "              #'model__penalty': ['l1','l2','elasticnet',None],         #LogisticRegression\n",
    "              #'model__C': [0.01, 0.1 , 1, 2],                           #LogisticRegression\n",
    "              #'model__solver': ['liblinear', 'sag'],                   #LogisticRegression\n",
    "              #'model__max_iter': [5000],                                #LogisticRegression\n",
    "              #'model__multi_class': ['ovr', 'multinomial'],             #LogisticRegression\n",
    "              #'model__random_state': [42],                              #LogisticRegression\n",
    "              #'model__C': [1],                                    #SVM RBF\n",
    "              #'model__gamma': [1]                                      #SVM RBF\n",
    "               'model__hidden_layer_sizes': [(100, ),(100,50)], \n",
    "               'model__activation': ['relu'],\n",
    "               'model__solver': ['adam'], \n",
    "               'model__random_state': [42]\n",
    "}\n",
    "\n",
    "# Create GridSearchCV object\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1_macro', error_score='raise', verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "df_gridsearch = pd.DataFrame(grid_search.cv_results_)\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "\n",
    "#Create blank results dataframe - only do this once\n",
    "#selected_columns = ['Model','Dataset','Timestamp','mean_fit_time','mean_score_time','params','split0_test_score','split1_test_score','split2_test_score','split3_test_score','split4_test_score','std_test_score','mean_test_score']\n",
    "###results_df = pd.DataFrame(columns=selected_columns)\n",
    "###results_df.to_csv('/content/drive/MyDrive/Professionele ontwikkeling/Data Science/Explore Data Science Course/Sprint 6_Advanced Classification/Predict/advanced-classification-predict/notebook/Model results.csv', index=False)\n",
    "#new_data = pd.DataFrame(columns=selected_columns)\n",
    "#new_data[['mean_fit_time','mean_score_time','params','split0_test_score','split1_test_score',\n",
    "#         'split2_test_score','split3_test_score','split4_test_score','std_test_score','mean_test_score']] = df_gridsearch[['mean_fit_time',\n",
    "#         'mean_score_time','params','split0_test_score','split1_test_score','split2_test_score','split3_test_score','split4_test_score','std_test_score','mean_test_score']]\n",
    "#new_data['Model'] = model_name\n",
    "#new_data['Dataset'] = size\n",
    "#new_data['Timestamp']= datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "#new_data.to_csv('/content/drive/MyDrive/Professionele ontwikkeling/Data Science/Explore Data Science Course/Sprint 6_Advanced Classification/Predict/advanced-classification-predict/notebook/Model results.csv', mode='a', header=False, index=False)\n",
    "###filename = f'/content/drive/MyDrive/Professionele ontwikkeling/Data Science/Explore Data Science Course/Sprint 6_Advanced Classification/Predict/advanced-classification-predict/notebook/results_{timestamp}.xlsx'\n",
    "###df_gridsearch.to_excel(filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "K3uO8OX7O3_7",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset original shape: (15819, 3)\n",
      "Dataset original class distribution: {1: 0.539, 2: 0.23, 0: 0.149, -1: 0.082}\n"
     ]
    }
   ],
   "source": [
    "# FIT ONE MODEL TO CHECK AGAINST GRIDSEARCH\n",
    "df_train, size = load_dataset('jupyter',1)\n",
    "df_train['sentiment'] = transform_categorical_labels(df_train['sentiment'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train['message'], df_train['sentiment'], test_size=0.2, random_state=42, stratify=df_train['sentiment'])\n",
    "\n",
    "preprocessing_steps = [\n",
    "                       ('noise_removal', NoiseRemover()),\n",
    "                       ('emoticon_convertion', EmoticonConverter()),\n",
    "                       ('punctuation_removal', PunctuationRemover()),\n",
    "                       ('tokenization', Tokenizer(type='TweetTokenizer')),\n",
    "                       ('stopword_removal', StopwordRemover()),\n",
    "                       ('lemmatization', Lemmatizer()),\n",
    "                       ('vectorization', Vectorize(max_features=20000,max_df=0.6,ngram_range=(1,15), type='tfidf')),\n",
    "                       #('randomoversampler', RandomOverSampler(sampling_strategy='auto')),\n",
    "                       #('randomundersampler', RandomUnderSampler(sampling_strategy='auto')),\n",
    "                       ('smote', SMOTE()),\n",
    "                       ('scaler', Scaler(type='robust')),\n",
    "                       ('feature_selection', FeatureSelector(percentile=2, type='anova_f'))\n",
    "                       ]\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "model = MLPClassifier(hidden_layer_sizes=(10000,), \n",
    "                     activation='relu',\n",
    "                     solver='adam', \n",
    "                     random_state=42,\n",
    "                     max_iter=10000)\n",
    "pipeline = Pipeline(preprocessing_steps + [('model', model)])\n",
    "#scorer = make_scorer(f1_score, average='macro')\n",
    "#f1_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring=scorer)\n",
    "#print(\"Mean F1 macro score:\", f1_scores.mean())\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "#model_stats = pipeline.named_steps['model']\n",
    "\n",
    "# Obtain the number of features used by the model\n",
    "#attributes = model_stats.__dict__\n",
    "#shape = attributes['shape_fit_']\n",
    "#cw = attributes['class_weight_']\n",
    "#print(f'Shape: {shape}')\n",
    "#print(f'class weight: {cw}')\n",
    "\n",
    "# Print all attributes\n",
    "#for attr, value in attributes.items():\n",
    "#    print(attr, \":\", value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset original shape: (15819, 3)\n",
      "Dataset original class distribution: {1: 0.539, 2: 0.23, 0: 0.149, -1: 0.082}\n",
      "(12655, 19549)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nX_train, y_train = smote.fit_resample(X_train, y_train)\\nX_train = scaler.fit_transform(X_train)\\nX_train = feature_selection.fit_transform(X_train, y_train)\\n\\n\\nX_test = noise_removal.transform(X_test)\\nX_test = emoticon_convertion.transform(X_test)\\nX_test = punctuation_removal.transform(X_test)\\nX_test = tokenization.transform(X_test)\\nX_test = stopword_removal.transform(X_test)\\nX_test = lemmatization.transform(X_test)\\nX_test = vectorization.transform(X_test)\\nX_test = scaler.transform(X_test)\\nX_test = feature_selection.transform(X_test, y_train)\\n\\nmodel = KNeighborsClassifier(n_neighbors=3)\\nmodel.fit(X_train, y_train)\\ny_pred = model.predict(X_test)\\nf1_macro = f1_score(y_test, y_pred, average=\\'macro\\')\\nprint(\"F1 Macro Score:\", f1_macro)\\n\\n# Obtain the number of features used by the model\\nattributes = model.__dict__\\n#shape = attributes[\\'shape_fit_\\']\\n#cw = attributes[\\'class_weight_\\']\\n#print(f\\'Shape: {shape}\\')\\n#print(f\\'class weight: {cw}\\')\\n\\n# Print all attributes\\n#for attr, value in attributes.items():\\n#    print(attr, \":\", value)\\n'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train, size = load_dataset('jupyter',0.2)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train['message'], df_train['sentiment'], test_size=0.2, random_state=42, stratify=df_train['sentiment'])\n",
    "\n",
    "'''\n",
    "## CUSTOM CLASS RESAMPLING\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "class_min1 = df_train[df_train['sentiment']==-1]\n",
    "class_0 = df_train[df_train['sentiment']==0]\n",
    "class_1 = df_train[df_train['sentiment']==1]\n",
    "class_2 = df_train[df_train['sentiment']==2]\n",
    "balance = len(df_train) // 4 # The number of samples that will result in class balance\n",
    "df_train_class1_resampled = resample(class_1,\n",
    "                            replace=False, # sample without replacement (no need to duplicate observations)\n",
    "                            n_samples=balance, # make all classes equal\n",
    "                            random_state=27) # reproducible results\n",
    "df_train_classmin1_resampled = resample(class_min1,\n",
    "                            replace=True, # sample with replacement (we need to duplicate observations)\n",
    "                            n_samples=balance, # make all classes equal\n",
    "                            random_state=27) # reproducible results\n",
    "df_train_class0_resampled = resample(class_0,\n",
    "                            replace=True, # sample with replacement (we need to duplicate observations)\n",
    "                            n_samples=balance, # make all classes equal\n",
    "                            random_state=27) # reproducible results\n",
    "df_train_class2_resampled = resample(class_2,\n",
    "                            replace=True, # sample with replacement (we need to duplicate observations)\n",
    "                            n_samples=balance, # make all classes equal\n",
    "                            random_state=27) # reproducible results\n",
    "\n",
    "df_train.reset_index(drop=True, inplace=True) # Reset index before upsampling\n",
    "df_train = pd.concat([df_train_class1_resampled, df_train_classmin1_resampled,\n",
    "                                df_train_class0_resampled, df_train_class2_resampled])\n",
    "df_train.set_index(df_train.index, inplace=True) # Set the default integer index as the new index after upsampling\n",
    "\n",
    "# Check new class counts\n",
    "print(df_train['sentiment'].value_counts())\n",
    "X_train = df_train['message'].squeeze()\n",
    "y_train = df_train['sentiment'].squeeze()\n",
    "'''\n",
    "\n",
    "noise_removal = NoiseRemover()\n",
    "emoticon_convertion = EmoticonConverter()\n",
    "punctuation_removal = PunctuationRemover()\n",
    "tokenization = Tokenizer(type='TweetTokenizer')\n",
    "stopword_removal = StopwordRemover()\n",
    "lemmatization = Lemmatizer()\n",
    "vectorization = Vectorize(max_df=0.01,ngram_range = (1,1), max_features=None, type='tfidf')\n",
    "smote = SMOTE()\n",
    "scaler = Scaler(type='robust')\n",
    "feature_selection = FeatureSelector(percentile=60, type='anova_f')\n",
    "X_train = noise_removal.fit_transform(X_train)\n",
    "X_train = emoticon_convertion.fit_transform(X_train)\n",
    "X_train = punctuation_removal.fit_transform(X_train)\n",
    "X_train = tokenization.fit_transform(X_train)\n",
    "X_train = stopword_removal.fit_transform(X_train)\n",
    "X_train = lemmatization.fit_transform(X_train)\n",
    "X_train = vectorization.fit_transform(X_train)\n",
    "print(X_train.shape)\n",
    "\"\"\"\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_train = feature_selection.fit_transform(X_train, y_train)\n",
    "\n",
    "\n",
    "X_test = noise_removal.transform(X_test)\n",
    "X_test = emoticon_convertion.transform(X_test)\n",
    "X_test = punctuation_removal.transform(X_test)\n",
    "X_test = tokenization.transform(X_test)\n",
    "X_test = stopword_removal.transform(X_test)\n",
    "X_test = lemmatization.transform(X_test)\n",
    "X_test = vectorization.transform(X_test)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_test = feature_selection.transform(X_test, y_train)\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "print(\"F1 Macro Score:\", f1_macro)\n",
    "\n",
    "# Obtain the number of features used by the model\n",
    "attributes = model.__dict__\n",
    "#shape = attributes['shape_fit_']\n",
    "#cw = attributes['class_weight_']\n",
    "#print(f'Shape: {shape}')\n",
    "#print(f'class weight: {cw}')\n",
    "\n",
    "# Print all attributes\n",
    "#for attr, value in attributes.items():\n",
    "#    print(attr, \":\", value)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "67oR1ZzJKWCY",
    "outputId": "576f9b31-5e31-4f0a-d9c7-e0f0595f82e7",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MODEL COMPARISON\n",
    "df_train, size = load_dataset(0.1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train['message'], df_train['sentiment'], test_size=0.2, random_state=42, stratify=df_train['sentiment'])\n",
    "preprocessing_steps = [\n",
    "                       ('noise_removal', NoiseRemover()),\n",
    "                       ('emoticon_convertion', EmoticonConverter()),\n",
    "                       ('punctuation_removal', PunctuationRemover()),\n",
    "                       ('tokenization', Tokenizer()),\n",
    "                       ('stopword_removal', StopwordRemover()),\n",
    "                       ('lemmatization', Lemmatizer()),\n",
    "                       ('vectorization', Vectorize()),\n",
    "                       ('smote', SMOTE()),\n",
    "                       ('scaler', Scaler()),\n",
    "                       ('feature_selection', FeatureSelector())\n",
    "                       ]\n",
    "\n",
    "models =        {\n",
    "                 'Multinomial Naive Bayes': MultinomialNB(),\n",
    "                 'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "                 'KNN': KNeighborsClassifier(n_neighbors=3, algorithm='ball_tree', leaf_size=30, metric='minkowski', p=2, weights='distance'),\n",
    "                 'SVC - linear': SVC(kernel=\"linear\", C=0.025),\n",
    "                 'SVC - RBF': SVC(gamma=1, C=1),\n",
    "                 'DecisionTree': DecisionTreeClassifier(max_depth=5),\n",
    "                 'RandomForest': RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1, random_state=42),\n",
    "                 'AdaBoost': AdaBoostClassifier(random_state=42, n_estimators=200),\n",
    "\n",
    "                }\n",
    "\n",
    "# Create the pipelines\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipeline = Pipeline(preprocessing_steps  + [('model', model)])\n",
    "    param_grid = {\n",
    "              'tokenization__type': ['TweetTokenizer'],#'TreebankWordTokenizer'],\n",
    "              'vectorization__type': ['tfidf','count'],\n",
    "              'vectorization__max_df': [0.3, 0.5,0.75],\n",
    "              'vectorization__min_df': [2,10],\n",
    "              'vectorization__ngram_range': [(1,2),(1,5),(1,7)],\n",
    "              'vectorization__max_features': [5000,10000],\n",
    "              'scaler__type': ['robust'],#'minmax','maxabs'],\n",
    "              'feature_selection__type': ['anova_f','mutualinfo'],\n",
    "              'feature_selection__percentile': [75,50],\n",
    "                }\n",
    "\n",
    "    # Create GridSearchCV object\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1_macro', error_score='raise', verbose=2)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    df_gridsearch = pd.DataFrame(grid_search.cv_results_)\n",
    "    #Create blank results dataframe - only do this once\n",
    "    selected_columns = ['Model','Dataset','Timestamp','mean_fit_time','mean_score_time','params','split0_test_score','split1_test_score','split2_test_score','split3_test_score','split4_test_score','std_test_score','mean_test_score']\n",
    "    #results_df = pd.DataFrame(columns=selected_columns)\n",
    "    #results_df.to_csv('/content/drive/MyDrive/Professionele ontwikkeling/Data Science/Explore Data Science Course/Sprint 6_Advanced Classification/Predict/advanced-classification-predict/notebook/Model_Comparison results.csv', index=False)\n",
    "    new_data = pd.DataFrame(columns=selected_columns)\n",
    "    new_data[['mean_fit_time','mean_score_time','params','split0_test_score','split1_test_score',\n",
    "         'split2_test_score','split3_test_score','split4_test_score','std_test_score','mean_test_score']] = df_gridsearch[['mean_fit_time',\n",
    "         'mean_score_time','params','split0_test_score','split1_test_score','split2_test_score','split3_test_score','split4_test_score','std_test_score','mean_test_score']]\n",
    "    new_data['Model'] = model_name\n",
    "    new_data['Dataset'] = size\n",
    "    new_data['Timestamp']= datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    new_data.to_csv('/content/drive/MyDrive/Professionele ontwikkeling/Data Science/Explore Data Science Course/Sprint 6_Advanced Classification/Predict/advanced-classification-predict/notebook/Model_Comparison results.csv', mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5fuiWQY24err"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Evaluate the pipeline\n",
    "accuracy = pipeline.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "model_stats = pipeline.named_steps['model']\n",
    "\n",
    "# Obtain the number of features used by the model\n",
    "attributes = model_stats.__dict__\n",
    "\n",
    "# Print all attributes\n",
    "for attr, value in attributes.items():\n",
    "    print(attr, \":\", value)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FaIL1W_gpHrG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQoIB6k0XiRF"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f'G:/MyDrive/Professionele ontwikkeling/Data Science/Explore Data Science Course/Sprint 6_Advanced Classification/Predict/advanced-classification-predict/notebook/dawieloots_predict_gridsearch_{timestamp}.csv'\n",
    "df_gridsearch.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_lxVdk5DJ4Ci"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(df_gridsearch)\n",
    "print(grid_search.get_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6fbMy5RktY2"
   },
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Custom transformer to wrap SMOTE\n",
    "\n",
    "class ResampleAndFeatureSelectTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, k=1000, score_func=chi2, k_neighbors=5):\n",
    "        self.k = k\n",
    "        self.score_func = score_func\n",
    "        self.k_neighbors = k_neighbors\n",
    "        self.feature_selector = SelectKBest(score_func=self.score_func, k=self.k)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Print the shape of X_train before preprocessing\n",
    "               \n",
    "        # Select features from the resampled data\n",
    "        self.feature_selector.fit(X, y)\n",
    "        \n",
    "        # Print the shape of X_train after preprocessing\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Select features from the input data\n",
    "        X_selected = self.feature_selector.transform(X)\n",
    "        return X_selected\n",
    "\n",
    "# Load the 20 Newsgroups dataset\n",
    "data = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "data_subset = {\n",
    "    'data': data.data[:1000],\n",
    "    'target': data.target[:1000],\n",
    "    'target_names': data.target_names\n",
    "}\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_subset['data'], data_subset['target'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Define preprocessing steps\n",
    "preprocessing_steps = [\n",
    "    # Text preprocessing\n",
    "    ('vectorizer', TfidfVectorizer()),  # Convert text data into numerical vectors\n",
    "    ('smote', SMOTE()),\n",
    "    ('resample_and_feature_select', ResampleAndFeatureSelectTransformer(k=1000, score_func=chi2)),  # Resample and select top k features\n",
    "    \n",
    "]\n",
    "\n",
    "# Define the model\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(preprocessing_steps + [('model', model)])\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the pipeline\n",
    "accuracy = pipeline.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "model = pipeline.named_steps['model']\n",
    "\n",
    "# Obtain the number of features used by the model\n",
    "attributes = model.__dict__\n",
    "\n",
    "# Print all attributes\n",
    "for attr, value in attributes.items():\n",
    "    print(attr, \":\", value)\n",
    "#n_features = model.n_features_\n",
    "#print(\"Number of features used by the model:\", n_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AyoIXSk4rU58"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

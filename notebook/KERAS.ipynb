{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 601,
     "status": "ok",
     "timestamp": 1712996860193,
     "user": {
      "displayName": "Dawie Loots",
      "userId": "00798578190767538914"
     },
     "user_tz": -120
    },
    "id": "h20rlqi05cIw",
    "outputId": "6703fe83-3a4a-4dba-c61b-8a4ada2ae66b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dawie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dawie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Libraries for data loading, data manipulation and data visulisation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import chardet # To provide a best estimate of the encoding that was used in the text data\n",
    "import io # For string operations\n",
    "%matplotlib inline\n",
    "\n",
    "# Libraries for data preparation and model building\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer, TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import datetime\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import math\n",
    "import re\n",
    "from sklearn.utils import resample\n",
    "from scipy.sparse import vstack\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import feature_selection\n",
    "from sklearn.feature_selection import SelectPercentile, SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score, log_loss\n",
    "from sklearn.model_selection import GridSearchCV, KFold, StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE,SMOTENC\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2hMYSMSB6fHp"
   },
   "outputs": [],
   "source": [
    "# LOAD DATASET\n",
    "\n",
    "def transform_categorical_labels(data):\n",
    "    # Dictionary mapping numerical categories to labels\n",
    "    label_map = {\n",
    "        2: 'News',\n",
    "        1: 'Pro',\n",
    "        0: 'Neutral',\n",
    "        -1: 'Anti'\n",
    "    }\n",
    "    transformed_data = data.map(label_map)\n",
    "    return transformed_data\n",
    "\n",
    "def class_distribution(data):\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        unique_classes, class_counts = data.iloc[:, 0].value_counts().index, data.iloc[:, 0].value_counts().values\n",
    "    elif isinstance(data, pd.Series):\n",
    "        unique_classes, class_counts = data.value_counts().index, data.value_counts().values\n",
    "    class_dict = dict(zip(unique_classes, class_counts))\n",
    "    return class_dict\n",
    "\n",
    "def load_dataset(environment, size, change_labels):\n",
    "    if environment == 'colab':\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        csv_file = '/content/drive/MyDrive/Professionele ontwikkeling/Data Science/Explore Data Science Course/Sprint 6_Advanced Classification/Predict/advanced-classification-predict/data/train.csv'\n",
    "    else:\n",
    "        csv_file = r'G:\\My Drive\\Professionele ontwikkeling\\Data Science\\Explore Data Science Course\\Sprint 6_Advanced Classification\\Predict\\advanced-classification-predict\\data\\train.csv'\n",
    "    df = pd.read_csv(csv_file)\n",
    "    if change_labels:\n",
    "        df['sentiment'] = transform_categorical_labels(df['sentiment'])\n",
    "    sample_size = int(len(df) * size)\n",
    "    if size == 1:\n",
    "        pass\n",
    "    else:\n",
    "        X = df.drop(columns=['sentiment']).copy()\n",
    "        y = df.sentiment.copy()\n",
    "        X_sample, _, y_sample, _ = train_test_split(X, y, train_size=sample_size, stratify=y, random_state=42)\n",
    "        df = pd.concat([X_sample, y_sample], axis=1)\n",
    "\n",
    "#       np.random.seed(42)  # OLD CODE\n",
    "#       pd.read_csv(csv_file).shape[0] - n  # OLD CODE\n",
    "#        df = pd.read_csv(csv_file, skiprows=lambda i: i > 0 and np.random.rand() > n / (i + 1), nrows=n)  # OLD CODE\n",
    "\n",
    "    class_dict = class_distribution(df['sentiment'])\n",
    "\n",
    "    return df, sample_size, class_dict\n",
    "\n",
    "\n",
    "# def load_full_dataset():  # OLD CODE\n",
    "#   df = pd.read_csv(csv_file)  # OLD CODE\n",
    "#   return df, 'full'  # OLD CODE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUVu-k0BXiQ3"
   },
   "source": [
    "df_train = pd.read_csv('G:/MyDrive/Professionele ontwikkeling/Data Science/Explore Data Science Course/Sprint 6_Advanced Classification/Predict/advanced-classification-predict/data/train.csv')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1rI10jOolRbZ"
   },
   "outputs": [],
   "source": [
    "class NoiseRemover(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        pattern_url = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'   # Find all hyperlinks\n",
    "        subs_url = r''\n",
    "        X_transformed = pd.Series(X).replace(to_replace = pattern_url, value = subs_url, regex = True)\n",
    "        return X_transformed\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "npvvupgdqXbC"
   },
   "outputs": [],
   "source": [
    "class EmoticonConverter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        emoticon_dictionary = {':\\)': 'smiley_face_emoticon',\n",
    "                               ':\\(': 'frowning_face_emoticon',\n",
    "                               ':D': 'grinning_face_emoticon',\n",
    "                               ':P': 'sticking_out_tongue_emoticon',\n",
    "                               ';\\)': 'winking_face_emoticon',\n",
    "                               ':o': 'surprised_face_emoticon',\n",
    "                               ':\\|': 'neutral_face_emoticon',\n",
    "                               ':\\'\\)': 'tears_of_joy_emoticon',\n",
    "                               ':\\'\\(': 'crying_face_emoticon'}\n",
    "        X_transformed = X.replace(emoticon_dictionary, regex=True)\n",
    "        return X_transformed\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "VTRod0QUsOL7"
   },
   "outputs": [],
   "source": [
    "class PunctuationRemover(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        def expand_contractions(text):\n",
    "            contractions = {\"'t\": \" not\",\"'s\": \" is\",\"'re\": \" are\",\"'ll\": \" will\", \"'m\": \" am\"}\n",
    "            pattern = re.compile(r\"\\b(\" + \"|\".join(re.escape(key) for key in contractions.keys()) + r\")\\b\")\n",
    "            text = re.sub(r\"n't\\b\", \" not\", text) # Replace \"n't\" with \" not\"\n",
    "            text = pattern.sub(lambda match: contractions[match.group(0)], text) # Replace all other contractions except for \"n't\"\n",
    "            return text\n",
    "\n",
    "        def remove_punctuation(text):\n",
    "            return ''.join([l.lower() for l in text if l not in string.punctuation])\n",
    "\n",
    "        X_transformed = X.apply(lambda x: expand_contractions(x))\n",
    "        X_transformed = X_transformed.apply(lambda x: remove_punctuation(x))\n",
    "        return X_transformed\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "b7cme7kUuyIt"
   },
   "outputs": [],
   "source": [
    "class Tokenizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, type=None):\n",
    "        self.type = type\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if self.type == 'TweetTokenizer':\n",
    "            tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True)\n",
    "        elif self.type == 'TreebankWordTokenizer':\n",
    "            tokenizer = TreebankWordTokenizer()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid tokenizer type. Choose either 'TweetTokenizer' or 'TreebankWordTokenizer'\")\n",
    "        X_transformed = X.apply(tokenizer.tokenize)\n",
    "        return X_transformed\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "rKXIUNy-TF-b"
   },
   "outputs": [],
   "source": [
    "class GarbageRemover(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, type=None):\n",
    "        self.type = type\n",
    "        self.pattern = re.compile(r'^[a-zA-Z\\U0001F300-\\U0001FAD6\\U0001F004-\\U0001F251]*[^a-zA-Z\\U0001F300-\\U0001FAD6\\U0001F004-\\U0001F251]+[a-zA-Z\\U0001F300-\\U0001FAD6\\U0001F004-\\U0001F251]*$')\n",
    "    def is_garbage(self, word):\n",
    "        \"\"\"\n",
    "        Define a regular expression pattern to match garbage words\n",
    "        Garbage is defined as words starting with anything but a letter, or\n",
    "        words containing strange characters.  Emojis are not considered garbage\n",
    "        \"\"\"\n",
    "        return bool(self.pattern.match(word))\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        transformed_words = [[word for word in word_list if not self.is_garbage(word)] or ['blankpost'] for word_list in X]\n",
    "        return pd.Series(transformed_words)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-puz8STKxpt3"
   },
   "outputs": [],
   "source": [
    "class StopwordRemover(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = X.apply(lambda tokens: [t for t in tokens if t.lower() not in self.stop_words])\n",
    "        return X_transformed\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1miNSgYK4zfz"
   },
   "outputs": [],
   "source": [
    "class Lemmatizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, pos=None):\n",
    "        self.pos = pos\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if self.pos not in ['v','n', 'a', 'r']:\n",
    "            raise ValueError('Invalid pos type. Choose either a, n, r or v')\n",
    "        else:\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            X_transformed = X.apply(lambda tokens: [lemmatizer.lemmatize(word, pos=self.pos) for word in tokens])\n",
    "        return X_transformed\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "XYw7dmak74hm"
   },
   "outputs": [],
   "source": [
    "class Vectorize(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, type=None, max_df=None, min_df=None, ngram_range=None, max_features=None):\n",
    "        self.type = type\n",
    "        self.max_df = max_df\n",
    "        self.min_df = min_df\n",
    "        self.ngram_range = ngram_range\n",
    "        self.max_features = max_features\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.type == 'count':\n",
    "            self.vectorizer = CountVectorizer(max_features=self.max_features, lowercase=True,\n",
    "                                              max_df=self.max_df, min_df=self.min_df,\n",
    "                                              ngram_range=self.ngram_range)\n",
    "        elif self.type == 'tfidf':\n",
    "            self.vectorizer = TfidfVectorizer(max_features=self.max_features, lowercase=True,\n",
    "                                              max_df=self.max_df, min_df=self.min_df,\n",
    "                                              ngram_range=self.ngram_range)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid vectorizer type. Choose either 'count' or 'tfidf'\")\n",
    "\n",
    "        X_joined = [' '.join(tokens) for tokens in X]\n",
    "        self.vectorizer.fit(X_joined)\n",
    "        return self.vectorizer.fit(X_joined)\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_joined = [' '.join(tokens) for tokens in X]\n",
    "        return self.vectorizer.transform(X_joined)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "TP0o7OpfGpg3"
   },
   "outputs": [],
   "source": [
    "class Scaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, type=None):\n",
    "        self.type = type\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.type == 'robust':\n",
    "            self.scaler = RobustScaler(with_centering=False)\n",
    "        elif self.type == 'minmax':\n",
    "            self.scaler = MinMaxScaler(with_centering=False)\n",
    "        elif self.type == 'maxabs':\n",
    "            self.scaler = MaxAbsScaler(with_centering=False)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid scaler type. Choose between 'robust', 'minmax' or 'maxabs'.\")\n",
    "        self.scaler.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return self.scaler.transform(X)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "R5UyERuHxKKI"
   },
   "outputs": [],
   "source": [
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, type=None, percentile=None):\n",
    "        self.type = type\n",
    "        self.percentile = percentile\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.type == 'mutualinfo':\n",
    "            self.selector = SelectPercentile(score_func=mutual_info_classif, percentile=self.percentile)\n",
    "        elif self.type == 'anova_f':\n",
    "            self.selector = SelectPercentile(score_func=f_classif, percentile=self.percentile)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid selector type. Choose between 'mutualinfo' or 'anova_f'.\")\n",
    "        self.selector.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return self.selector.transform(X)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhW2csKbHUnB"
   },
   "source": [
    "X = df_train['message']\n",
    "y = df_train['sentiment']\n",
    "noise = NoiseRemover()\n",
    "X = noise.fit_transform(X)\n",
    "emoti = EmoticonConverter()\n",
    "X = emoti.fit_transform(X)\n",
    "punc = PunctuationRemover()\n",
    "X = punc.fit_transform(X)\n",
    "token = Tokenizer(type='TweetTokenizer')\n",
    "X = token.fit_transform(X)\n",
    "garbage = GarbageRemover()\n",
    "X = garbage.fit_transform(X)\n",
    "stop = StopwordRemover()\n",
    "X = stop.fit_transform(X)\n",
    "print(X)\n",
    "\n",
    "\"\"\"\n",
    "lemma = Lemmatizer(pos='v')\n",
    "X = lemma.fit_transform(X)\n",
    "\n",
    "words_list = X.explode().tolist()\n",
    "\n",
    "\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_word_cloud(words, title):\n",
    "    # Join the words into a single string\n",
    "    text = ' '.join(words)\n",
    "\n",
    "    # Generate a word cloud\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "\n",
    "    # Plot the word cloud\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "create_word_cloud(words_list, \"All Words\")\n",
    "\n",
    "def classify_words(words):\n",
    "    actual_words = []\n",
    "    garbage_words = []\n",
    "\n",
    "    # Regular expression pattern to match a word\n",
    "    word_pattern = re.compile(r'^[a-zA-Z]+$')\n",
    "\n",
    "    for word in words:\n",
    "        # Check if the word matches the pattern\n",
    "        if word_pattern.match(word) and word:\n",
    "            actual_words.append(word)\n",
    "        else:\n",
    "            garbage_words.append(word)\n",
    "\n",
    "    return actual_words, garbage_words\n",
    "\n",
    "# Example usage:\n",
    "actual, garbage = classify_words(words_list)\n",
    "print(f'Actual words: {len(actual)}')\n",
    "print(f'Garbage words: {len(garbage)}')\n",
    "\n",
    "create_word_cloud(actual, \"Actual Words\")\n",
    "create_word_cloud(garbage, \"Garbage\")\n",
    "\n",
    "import csv\n",
    "\n",
    "\n",
    "def is_garbage(word):\n",
    "    # Define a regular expression pattern to match garbage words\n",
    "    pattern = r'^[a-zA-Z\\U0001F300-\\U0001FAD6\\U0001F004-\\U0001F251]*[^a-zA-Z\\U0001F300-\\U0001FAD6\\U0001F004-\\U0001F251]+[a-zA-Z\\U0001F300-\\U0001FAD6\\U0001F004-\\U0001F251]*$'\n",
    "    # Check if the word matches the pattern\n",
    "    return bool(re.match(pattern, word))\n",
    "\n",
    "def count_garbage_words(words):\n",
    "    garbage_count = {}\n",
    "\n",
    "    for word in words:\n",
    "        if is_garbage(word):\n",
    "            if word in garbage_count:\n",
    "                garbage_count[word] += 1\n",
    "            else:\n",
    "                garbage_count[word] = 1\n",
    "\n",
    "    return garbage_count\n",
    "\n",
    "def export_to_csv(garbage_counts, filename):\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['Garbage Word', 'Count']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        writer.writeheader()\n",
    "        for word, count in garbage_counts.items():\n",
    "            writer.writerow({'Garbage Word': word, 'Count': count})\n",
    "\n",
    "# Example usage:\n",
    "garbage_counts = count_garbage_words(words_list)\n",
    "\n",
    "# Export to CSV\n",
    "export_to_csv(garbage_counts, '/content/drive/MyDrive/Professionele ontwikkeling/Data Science/Explore Data Science Course/Sprint 6_Advanced Classification/Predict/advanced-classification-predict/notebook/garbagewords.csv')\n",
    "\n",
    "\n",
    "def words_summary(words):\n",
    "    summary = [0] * (max(len(word) for word in words) + 1)\n",
    "\n",
    "    # Iterate through the words\n",
    "    for word in words:\n",
    "        # Get the length of the word\n",
    "        length = len(word)\n",
    "        # Increment the count for the corresponding length\n",
    "        summary[length] += 1\n",
    "\n",
    "    return summary\n",
    "\n",
    "# Example usage:\n",
    "summary = words_summary(words_list)\n",
    "\n",
    "# Print the summary\n",
    "print(\"Summary of words per length:\")\n",
    "for length, count in enumerate(summary):\n",
    "    if count > 0:\n",
    "        print(f\"Length {length}: {count} words\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "executionInfo": {
     "elapsed": 40077,
     "status": "ok",
     "timestamp": 1712996901007,
     "user": {
      "displayName": "Dawie Loots",
      "userId": "00798578190767538914"
     },
     "user_tz": -120
    },
    "id": "QkpIKZ6Nfd7M",
    "outputId": "7a8df809-cdc2-4492-d0c6-0ea00806ae8b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5316</th>\n",
       "      <td>RT @Dwarfclone: @DocThompsonShow #WhatILearned...</td>\n",
       "      <td>701646</td>\n",
       "      <td>Anti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3405</th>\n",
       "      <td>RT @WilDonnelly: USDA chief scientist nominee ...</td>\n",
       "      <td>668314</td>\n",
       "      <td>Pro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12992</th>\n",
       "      <td>EPA chief unconvinced on CO2 link to global wa...</td>\n",
       "      <td>518811</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10114</th>\n",
       "      <td>RT @UNDP: #Africa added least to global #GHG e...</td>\n",
       "      <td>551647</td>\n",
       "      <td>Pro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13590</th>\n",
       "      <td>RT @SpiritualSmoker: it's absolutely disgustin...</td>\n",
       "      <td>350269</td>\n",
       "      <td>Pro</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 message  tweetid sentiment\n",
       "5316   RT @Dwarfclone: @DocThompsonShow #WhatILearned...   701646      Anti\n",
       "3405   RT @WilDonnelly: USDA chief scientist nominee ...   668314       Pro\n",
       "12992  EPA chief unconvinced on CO2 link to global wa...   518811      News\n",
       "10114  RT @UNDP: #Africa added least to global #GHG e...   551647       Pro\n",
       "13590  RT @SpiritualSmoker: it's absolutely disgustin...   350269       Pro"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train, size, class_dictionary = load_dataset('jupyter',0.1,change_labels=True)\n",
    "# Next section is to calculate class imbalance, for us in randomover and randomundersampling\n",
    "\n",
    "number_classes = len(class_dictionary)\n",
    "majority = max(class_dictionary.values())\n",
    "average_class_count = int(size / number_classes)\n",
    "oversampling_strategy = {key: max(average_class_count, count)  for key, count in class_dictionary.items()}\n",
    "undersampling_strategy = {key: average_class_count  for key, count in oversampling_strategy.items()}\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3pDtr9uCNIfh"
   },
   "outputs": [],
   "source": [
    "## FOR XGBOOST ONLY\n",
    "\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "#label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the string labels to integer labels\n",
    "#df_train[\"sentiment\"] = label_encoder.fit_transform(df_train[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34871,
     "status": "ok",
     "timestamp": 1713000518414,
     "user": {
      "displayName": "Dawie Loots",
      "userId": "00798578190767538914"
     },
     "user_tz": -120
    },
    "id": "5fVqF62W5XKf",
    "outputId": "f6681028-65ba-4ca9-ab28-1e76aa19f341"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (1264,)\n",
      "Shape of y_train: (1264,)\n",
      "Shape of X_test: (317,)\n",
      "Shape of y_test: (317,)\n",
      "Distribution of y_train: {'Pro': 681, 'News': 291, 'Neutral': 188, 'Anti': 104}\n",
      "Distribution of y_test: {'Pro': 171, 'News': 73, 'Neutral': 47, 'Anti': 26}\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras.wrappers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscikit_learn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasClassifier\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_categorical\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras.wrappers'"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train['message'], df_train['sentiment'], test_size=0.2, random_state=42, stratify=df_train['sentiment'])\n",
    "print(f'Shape of X_train: {X_train.shape}')\n",
    "print(f'Shape of y_train: {y_train.shape}')\n",
    "print(f'Shape of X_test: {X_test.shape}')\n",
    "print(f'Shape of y_test: {y_test.shape}')\n",
    "print(f'Distribution of y_train: {class_distribution(y_train)}')\n",
    "print(f'Distribution of y_test: {class_distribution(y_test)}')\n",
    "\n",
    "# Defining preprocessing steps\n",
    "preprocessing_steps = [\n",
    "\n",
    "                       ('noise_removal', NoiseRemover()),\n",
    "                       ('emoticon_convertion', EmoticonConverter()),\n",
    "                       ('punctuation_removal', PunctuationRemover()),\n",
    "                       ('tokenization', Tokenizer()),\n",
    "                       ('garbageout', GarbageRemover()),\n",
    "                       ('stopword_removal', StopwordRemover()),\n",
    "                       ('lemmatization', Lemmatizer()),\n",
    "                       ('vectorization', Vectorize()),\n",
    "                       #('oversampler', RandomOverSampler(sampling_strategy=oversampling_strategy, random_state=42)),\n",
    "                       #('undersampler', RandomUnderSampler(sampling_strategy=undersampling_strategy, random_state=42)),\n",
    "                       ('smote', SMOTE()),\n",
    "                       ('scaler', Scaler()),\n",
    "                       ('feature_selection', FeatureSelector())\n",
    "                       ]\n",
    "\n",
    "\n",
    "# Create the pipelines\n",
    "#model = xgb.XGBClassifier()\n",
    "#model = AdaBoostClassifier()\n",
    "#model_name = 'Adaboost'\n",
    "#model = MultinomialNB()\n",
    "#model_name = 'MultinomialNB'\n",
    "#model = LogisticRegression()\n",
    "#model_name = 'Logis0ticRegression'\n",
    "#model = KNeighborsClassifier()\n",
    "#model_name = 'KNN'\n",
    "#model = LogisticRegression(C=1, max_iter=1000000,multi_class='multinomial', solver='sag', tol=0.001, penalty='l2', random_state=42)\n",
    "#model = SVC(C=100,gamma=0.1)  #THIS SEEMS TO BE THE BEST\n",
    "#model = SVC()\n",
    "#model_name='SVC RBF'\n",
    "#model = RandomForestClassifier()\n",
    "#model_name='RandomForest'\n",
    "#model = MLPClassifier(hidden_layer_sizes=(100, 50), activation='relu', solver='adam', alpha=0.0001,\n",
    "#                    batch_size='auto', learning_rate='constant', learning_rate_init=0.001,\n",
    "#                    max_iter=200, random_state=42)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))  # 3 classes, change as per your number of classes\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Wrap Keras model in a scikit-learn compatible classifier\n",
    "model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=10, verbose=0)\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "# Convert integers to one-hot encoded format\n",
    "y_train_one_hot = to_categorical(y_train_encoded)\n",
    "y_test_one_hot = to_categorical(y_test_encoded)\n",
    "y_train = y_train_one_hot\n",
    "y_test = y_test_one_hot\n",
    "\n",
    "pipeline = Pipeline(preprocessing_steps  + [('model', model)])\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'tokenization__type': ['TweetTokenizer'],#'TreebankWordTokenizer',],\n",
    "    'lemmatization__pos': ['v'],#,'n'],#,'v','r'],\n",
    "    'vectorization__type': ['count'],#'tfidf'],\n",
    "    'vectorization__max_df': [0.5],#, 0.5],#, 1.0],#,0.5],\n",
    "    'vectorization__min_df': [1],#, 2],\n",
    "    'vectorization__ngram_range': [(1,3)],# (1, 10)],\n",
    "    'vectorization__max_features': [None],\n",
    "    'scaler__type': ['robust'],  # 'minmax','maxabs'],\n",
    "    'smote__k_neighbors': [3],\n",
    "    'smote__random_state': [42],\n",
    "    'feature_selection__percentile': [50],\n",
    "    'feature_selection__type': ['anova_f'],\n",
    "\n",
    "\n",
    "             # Hyperparameters for the model\n",
    "\n",
    "              #'model__n_neighbors': [3,4,5],                                #KNN\n",
    "              #'model__weights': [ 'distance','uniform'],               #KNN\n",
    "              #'model__metric': ['euclidean', 'manhattan'],            #KNN\n",
    "              #'model__C': [100,10,1],                                  #SVM RBF\n",
    "              #'model__gamma': [0.1,0.001, 0.01,1],# 0.1]                                 #SVM RBF\n",
    "               #'model__penalty': ['l2'],#,'l1'],                            #LogisticRegression\n",
    "               #'model__C': [10],#,100,1,0.001,0.1,],                             #LogisticRegression\n",
    "               #'model__solver': ['sag'],#['liblinear']                     #LogisticRegression\n",
    "               #'model__multi_class': ['multinomial'],#'ovr',               #LogisticRegression\n",
    "               #'model__max_iter': [1000000],                               #LogisticRegression\n",
    "               #'model__tol': [0.0001],#, 0.0001],                                #LogisticRegression\n",
    "               #'model__n_jobs': [-1],                                           #LogisticRegression\n",
    "               #'model__random_state': [42],                                     #LogisticRegression\n",
    "               #'model__alpha': [0.1],#3, 0.5,],# 0.1,1,5,10],                               #Multinomial NB\n",
    "               #'model__fit_prior': [True],# False],                               #Multinomial NB\n",
    "               #'model__n_estimators': [500],                              #Adaboost\n",
    "               #'model__learning_rate': [0.1],#, 1, 5],                              #Adaboost\n",
    "               #'model__random_state': [42],                                     #Adaboost\n",
    "               #'model__algorithm': ['SAMME'],                                    #Adaboost\n",
    "               #'model__estimator': [LogisticRegression()]#,SVC(kernel='rbf', C=100,gamma=0.1)],DecisionTreeClassifier(), ],   #Adaboost\n",
    "\n",
    "                #'model__n_estimators': [100],#200],                               #XGBoost\n",
    "                #'model__max_depth': [30],#,6],                               #XGBoost\n",
    "                #'model__learning_rate': [0.3, 1],                               #XGBoost\n",
    "                #'model__subsample': [1.0],                               #XGBoost\n",
    "              }\n",
    "\n",
    "# Create GridSearchCV object\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1_macro', error_score='raise', verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "df_gridsearch = pd.DataFrame(grid_search.cv_results_)\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "selected_columns = ['Model','Dataset','Timestamp','mean_fit_time','mean_score_time','params','split0_test_score','split1_test_score',\n",
    "                    'split2_test_score','split3_test_score','split4_test_score',\n",
    "                    'std_test_score','mean_test_score']\n",
    "\n",
    "#Create blank results dataframe - only do this once\n",
    "###results_df = pd.DataFrame(columns=selected_columns)\n",
    "###results_df.to_csv('/content/drive/MyDrive/Professionele ontwikkeling/Data Science/Explore Data Science Course/Sprint 6_Advanced Classification/Predict/advanced-classification-predict/notebook/Model results.csv', index=False)\n",
    "new_data = pd.DataFrame(columns=selected_columns)\n",
    "\n",
    "new_data[['mean_fit_time','mean_score_time','params','split0_test_score','split1_test_score',\n",
    "         'split2_test_score','split3_test_score','split4_test_score',\n",
    "          'std_test_score','mean_test_score']] = df_gridsearch[['mean_fit_time',\n",
    "         'mean_score_time','params','split0_test_score','split1_test_score',\n",
    "         'split2_test_score','split3_test_score','split4_test_score',\n",
    "         'std_test_score','mean_test_score']]\n",
    "new_data['Model'] = model_name\n",
    "new_data['Dataset'] = size\n",
    "new_data['Timestamp']= datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "new_data.to_csv('/content/drive/MyDrive/Professionele ontwikkeling/Data Science/Explore Data Science Course/Sprint 6_Advanced Classification/Predict/advanced-classification-predict/notebook/Model results.csv', mode='a', header=False, index=False)\n",
    "\n",
    "\n",
    "# Do a stand-alon pipeline to see the full class report\n",
    "#pipeline.fit(X_train, y_train)\n",
    "#y_pred = pipeline.predict(X_test)\n",
    "#report = classification_report(y_test, y_pred)\n",
    "#print(\"Classification Report for test set:\")\n",
    "#print(report)\n",
    "#print('************************')\n",
    "\n",
    "#k_fold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# Perform k-fold cross-validation and generate predictions\n",
    "#predictions = cross_val_predict(pipeline, X_train, y_train, cv=k_fold)\n",
    "\n",
    "# Generate classification report\n",
    "#report = classification_report(y_train, predictions)\n",
    "\n",
    "# Print the classification report\n",
    "#print(\"Classification Report for CV:\\n\", report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1712663823289,
     "user": {
      "displayName": "Dawie Loots",
      "userId": "00798578190767538914"
     },
     "user_tz": -120
    },
    "id": "05LvXgXJt4iO",
    "outputId": "ce1f053d-164d-46fb-c479-085ca6843aba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature_selection__percentile': 50, 'feature_selection__type': 'anova_f', 'model__C': 10, 'model__gamma': 1, 'scaler__type': 'robust', 'tokenization__type': 'TweetTokenizer', 'vectorization__max_features': 10000, 'vectorization__type': 'tfidf'}\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5267,
     "status": "ok",
     "timestamp": 1712749253146,
     "user": {
      "displayName": "Dawie Loots",
      "userId": "00798578190767538914"
     },
     "user_tz": -120
    },
    "id": "K3uO8OX7O3_7",
    "outputId": "e66c80d8-b26a-4695-bd81-7e89ee3f7a3c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (852) in class 1 will be larger than the number of samples in the majority class (class #1 -> 544)\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0 0 0 ... 0 0 0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (852) in class 1 will be larger than the number of samples in the majority class (class #1 -> 545)\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0 0 0 ... 0 0 0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (852) in class 1 will be larger than the number of samples in the majority class (class #1 -> 545)\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0 0 0 ... 0 0 0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (852) in class 1 will be larger than the number of samples in the majority class (class #1 -> 545)\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0 0 0 ... 0 0 0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (852) in class 1 will be larger than the number of samples in the majority class (class #1 -> 545)\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0 0 0 ... 0 0 0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean F1 macro score: 0.27094834786524913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (852) in class 1 will be larger than the number of samples in the majority class (class #1 -> 681)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision_function_shape : ovr\n",
      "break_ties : False\n",
      "kernel : rbf\n",
      "degree : 3\n",
      "gamma : 1\n",
      "coef0 : 0.0\n",
      "tol : 0.001\n",
      "C : 1\n",
      "nu : 0.0\n",
      "epsilon : 0.0\n",
      "shrinking : True\n",
      "probability : False\n",
      "cache_size : 200\n",
      "class_weight : None\n",
      "verbose : False\n",
      "max_iter : -1\n",
      "random_state : None\n",
      "_sparse : True\n",
      "n_features_in_ : 1511\n",
      "class_weight_ : [1. 1. 1. 1.]\n",
      "classes_ : [-1  0  1  2]\n",
      "_gamma : 1\n",
      "support_ : [  0   1   3   4   6   7   8  10  11  12  13  16  17  18  19  22  23  25\n",
      "  26  27  28  29  30  31  32  33  36  38  39  42  43  44  45  46  48  49\n",
      "  50  51  52  53  54  55  56  57  58  59  60  62  63  64  65  66  67  70\n",
      "  72  73  74  76  77  78  79  81  82  84  85  86  88  89  91  92  93  94\n",
      "  95  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114\n",
      " 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132\n",
      " 133 134 135 136 137 138 139 140 141 143 144 145 146 147 148 149 150 151\n",
      " 152 153 154 155 157 158 159 160 161 162 163 164 165 166 167 168 169 170\n",
      " 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188\n",
      " 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206\n",
      " 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225\n",
      " 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243\n",
      " 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261\n",
      " 262 263 264 265 266 268 269 270 271 272 273 274 275 276 277 278 279 280\n",
      " 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298\n",
      " 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334\n",
      " 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352\n",
      " 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370\n",
      " 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388\n",
      " 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406\n",
      " 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424\n",
      " 425 426 427 428 429 430 432 433 434 435 436 437 438 439 440 441 442 443\n",
      " 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461\n",
      " 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479\n",
      " 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497\n",
      " 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515\n",
      " 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533\n",
      " 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551\n",
      " 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569\n",
      " 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 588 589 590\n",
      " 591 592 593 594 595 596 597 598 599 601 602 603 604 605 606 607 608 609\n",
      " 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627\n",
      " 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645\n",
      " 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663\n",
      " 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681\n",
      " 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699\n",
      " 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717\n",
      " 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735\n",
      " 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753\n",
      " 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771\n",
      " 772 773 774 775 776 777 778 779]\n",
      "support_vectors_ :   (0, 156)\t0.3779644730092272\n",
      "  (0, 280)\t0.3779644730092272\n",
      "  (0, 540)\t0.3779644730092272\n",
      "  (0, 788)\t0.3779644730092272\n",
      "  (0, 1393)\t0.3779644730092272\n",
      "  (0, 1445)\t0.3779644730092272\n",
      "  (0, 1489)\t0.3779644730092272\n",
      "  (1, 1147)\t0.7071067811865476\n",
      "  (1, 1310)\t0.7071067811865476\n",
      "  (3, 201)\t0.7071067811865476\n",
      "  (3, 427)\t0.7071067811865476\n",
      "  (4, 169)\t0.447213595499958\n",
      "  (4, 286)\t0.447213595499958\n",
      "  (4, 530)\t0.447213595499958\n",
      "  (4, 884)\t0.447213595499958\n",
      "  (4, 1453)\t0.447213595499958\n",
      "  (5, 20)\t0.447213595499958\n",
      "  (5, 957)\t0.447213595499958\n",
      "  (5, 969)\t0.447213595499958\n",
      "  (5, 1308)\t0.447213595499958\n",
      "  (5, 1331)\t0.447213595499958\n",
      "  (6, 425)\t0.3779644730092272\n",
      "  (6, 503)\t0.3779644730092272\n",
      "  (6, 837)\t0.3779644730092272\n",
      "  (6, 1177)\t0.3779644730092272\n",
      "  :\t:\n",
      "  (735, 237)\t0.447213595499958\n",
      "  (735, 658)\t0.447213595499958\n",
      "  (735, 671)\t0.447213595499958\n",
      "  (735, 1214)\t0.447213595499958\n",
      "  (735, 1397)\t0.447213595499958\n",
      "  (736, 325)\t0.5\n",
      "  (736, 363)\t0.5\n",
      "  (736, 1062)\t0.5\n",
      "  (736, 1429)\t0.5\n",
      "  (737, 388)\t0.7071067811865476\n",
      "  (737, 506)\t0.7071067811865476\n",
      "  (739, 282)\t0.5773502691896258\n",
      "  (739, 1492)\t0.5773502691896258\n",
      "  (739, 1502)\t0.5773502691896258\n",
      "  (740, 923)\t1.0\n",
      "  (744, 265)\t0.5773502691896258\n",
      "  (744, 1048)\t0.5773502691896258\n",
      "  (744, 1150)\t0.5773502691896258\n",
      "  (745, 75)\t0.3779644730092272\n",
      "  (745, 294)\t0.3779644730092272\n",
      "  (745, 646)\t0.3779644730092272\n",
      "  (745, 1081)\t0.3779644730092272\n",
      "  (745, 1162)\t0.3779644730092272\n",
      "  (745, 1243)\t0.3779644730092272\n",
      "  (745, 1377)\t0.3779644730092272\n",
      "intercept_ : [-1.09683649  0.075741    0.17215417  0.72010424  0.89890001  0.86659265]\n",
      "_n_support : [168 193 194 191]\n",
      "_probA : []\n",
      "_probB : []\n",
      "fit_status_ : 0\n",
      "_num_iter : [623 545 577 240 437 506]\n",
      "dual_coef_ :   (0, 0)\t0.656517640415225\n",
      "  (0, 1)\t1.0\n",
      "  (0, 2)\t0.0\n",
      "  (0, 3)\t1.0\n",
      "  (0, 4)\t1.0\n",
      "  (0, 5)\t1.0\n",
      "  (0, 6)\t0.656517640415225\n",
      "  (0, 7)\t0.0\n",
      "  (0, 8)\t1.0\n",
      "  (0, 9)\t0.8765610270242215\n",
      "  (0, 10)\t1.0\n",
      "  (0, 11)\t1.0\n",
      "  (0, 12)\t0.656517640415225\n",
      "  (0, 13)\t0.656517640415225\n",
      "  (0, 14)\t0.656517640415225\n",
      "  (0, 15)\t1.0\n",
      "  (0, 16)\t1.0\n",
      "  (0, 17)\t0.0\n",
      "  (0, 18)\t1.0\n",
      "  (0, 19)\t1.0\n",
      "  (0, 20)\t1.0\n",
      "  (0, 21)\t0.0\n",
      "  (0, 22)\t0.0\n",
      "  (0, 23)\t0.656517640415225\n",
      "  (0, 24)\t0.0\n",
      "  :\t:\n",
      "  (2, 721)\t-1.0\n",
      "  (2, 722)\t-1.0\n",
      "  (2, 723)\t-1.0\n",
      "  (2, 724)\t-1.0\n",
      "  (2, 725)\t-1.0\n",
      "  (2, 726)\t-0.6572981539273356\n",
      "  (2, 727)\t-1.0\n",
      "  (2, 728)\t-1.0\n",
      "  (2, 729)\t-1.0\n",
      "  (2, 730)\t-0.7070647050519031\n",
      "  (2, 731)\t-1.0\n",
      "  (2, 732)\t-1.0\n",
      "  (2, 733)\t-1.0\n",
      "  (2, 734)\t-1.0\n",
      "  (2, 735)\t-1.0\n",
      "  (2, 736)\t-1.0\n",
      "  (2, 737)\t-1.0\n",
      "  (2, 738)\t-1.0\n",
      "  (2, 739)\t-0.7070647050519031\n",
      "  (2, 740)\t-1.0\n",
      "  (2, 741)\t-1.0\n",
      "  (2, 742)\t-0.7771392773343454\n",
      "  (2, 743)\t-0.0\n",
      "  (2, 744)\t-1.0\n",
      "  (2, 745)\t-1.0\n",
      "shape_fit_ : (780, 1511)\n",
      "_intercept_ : [-1.09683649  0.075741    0.17215417  0.72010424  0.89890001  0.86659265]\n",
      "_dual_coef_ :   (0, 0)\t0.656517640415225\n",
      "  (0, 1)\t1.0\n",
      "  (0, 2)\t0.0\n",
      "  (0, 3)\t1.0\n",
      "  (0, 4)\t1.0\n",
      "  (0, 5)\t1.0\n",
      "  (0, 6)\t0.656517640415225\n",
      "  (0, 7)\t0.0\n",
      "  (0, 8)\t1.0\n",
      "  (0, 9)\t0.8765610270242215\n",
      "  (0, 10)\t1.0\n",
      "  (0, 11)\t1.0\n",
      "  (0, 12)\t0.656517640415225\n",
      "  (0, 13)\t0.656517640415225\n",
      "  (0, 14)\t0.656517640415225\n",
      "  (0, 15)\t1.0\n",
      "  (0, 16)\t1.0\n",
      "  (0, 17)\t0.0\n",
      "  (0, 18)\t1.0\n",
      "  (0, 19)\t1.0\n",
      "  (0, 20)\t1.0\n",
      "  (0, 21)\t0.0\n",
      "  (0, 22)\t0.0\n",
      "  (0, 23)\t0.656517640415225\n",
      "  (0, 24)\t0.0\n",
      "  :\t:\n",
      "  (2, 721)\t-1.0\n",
      "  (2, 722)\t-1.0\n",
      "  (2, 723)\t-1.0\n",
      "  (2, 724)\t-1.0\n",
      "  (2, 725)\t-1.0\n",
      "  (2, 726)\t-0.6572981539273356\n",
      "  (2, 727)\t-1.0\n",
      "  (2, 728)\t-1.0\n",
      "  (2, 729)\t-1.0\n",
      "  (2, 730)\t-0.7070647050519031\n",
      "  (2, 731)\t-1.0\n",
      "  (2, 732)\t-1.0\n",
      "  (2, 733)\t-1.0\n",
      "  (2, 734)\t-1.0\n",
      "  (2, 735)\t-1.0\n",
      "  (2, 736)\t-1.0\n",
      "  (2, 737)\t-1.0\n",
      "  (2, 738)\t-1.0\n",
      "  (2, 739)\t-0.7070647050519031\n",
      "  (2, 740)\t-1.0\n",
      "  (2, 741)\t-1.0\n",
      "  (2, 742)\t-0.7771392773343454\n",
      "  (2, 743)\t-0.0\n",
      "  (2, 744)\t-1.0\n",
      "  (2, 745)\t-1.0\n",
      "n_iter_ : [623 545 577 240 437 506]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0 0 0 ... 0 0 0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    }
   ],
   "source": [
    "scorer = make_scorer(f1_score, average='macro')\n",
    "f1_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring=scorer)\n",
    "print(\"Mean F1 macro score:\", f1_scores.mean())\n",
    "pipeline.fit(X_train, y_train)\n",
    "model_stats = pipeline.named_steps['model']\n",
    "\n",
    "# Obtain the number of features used by the model\n",
    "attributes = model_stats.__dict__\n",
    "\n",
    "# Print all attributes\n",
    "for attr, value in attributes.items():\n",
    "    print(attr, \":\", value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "67oR1ZzJKWCY",
    "outputId": "58e2e8e7-eb79-40fb-ee4d-22d29d2fd7d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   1.3s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   1.6s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   1.3s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.8s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.9s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   3.3s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   3.9s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.8s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.9s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   3.0s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   4.4s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.9s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.9s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   1.2s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   1.1s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   1.4s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   1.7s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   1.0s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.9s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.9s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   4.0s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   3.2s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.8s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.9s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   3.6s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   3.8s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.9s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.9s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   1.0s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   1.4s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   1.4s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   1.2s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.9s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.9s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   4.3s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.9s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.8s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   3.0s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   4.2s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   3.2s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   3.0s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.9s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.7s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.7s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   1.2s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   1.2s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   1.3s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   3.4s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   4.2s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   3.6s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.6s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.7s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.7s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   4.1s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.8s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.7s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.7s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.7s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   1.2s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   1.2s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   1.2s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.7s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.7s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.7s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   3.4s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   3.4s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.7s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.8s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.8s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   4.3s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.8s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.7s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.7s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.7s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.7s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.7s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   1.0s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   1.3s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   1.4s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   1.0s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.6s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.6s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.6s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   3.7s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   3.1s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.8s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.8s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   3.1s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   4.1s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.8s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   1.4s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   1.5s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   1.3s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   3.8s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.8s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.8s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   4.3s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.8s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.9s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.9s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   4.1s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   3.2s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   3.0s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   1.3s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   1.5s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   1.4s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.8s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.9s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   3.1s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   4.1s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.8s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.9s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.9s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   4.4s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.9s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.9s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   1.2s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   1.4s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   1.6s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.8s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.8s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   3.6s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   3.6s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   3.2s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.9s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   3.6s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   3.9s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.9s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   3.0s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.7s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.7s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.7s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   1.0s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   1.2s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   1.5s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.6s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.7s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   3.2s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   3.7s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.6s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.7s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.8s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   4.0s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   3.0s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.7s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.7s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.7s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.7s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.7s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   1.1s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   1.2s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   1.4s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.7s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.6s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.6s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   3.3s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   3.5s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.8s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.8s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.7s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   4.2s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.7s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.7s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.7s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.7s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.7s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   1.2s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   3.7s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.7s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.6s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.6s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   3.6s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   3.3s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.8s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.7s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   3.0s\n",
      "[CV] END feature_selection__percentile=50, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   4.1s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   4.3s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.8s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.8s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.8s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   3.7s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   3.7s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.9s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.9s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   3.5s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   3.9s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   1.1s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   4.1s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.8s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.9s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.9s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   4.3s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   3.0s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   3.0s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.9s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   4.0s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   3.4s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   1.1s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   1.6s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   3.6s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.8s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.8s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   3.1s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   4.1s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.9s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.9s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.9s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   4.5s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   3.0s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.7s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.7s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.7s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.7s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   1.3s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   3.6s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.7s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.6s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.7s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   3.8s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   3.0s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.7s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.7s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   3.1s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   3.9s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.7s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.7s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.7s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.7s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   3.4s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   3.5s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.6s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   3.0s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.7s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   4.3s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.7s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.7s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.8s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   3.7s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   1.3s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.7s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.7s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   0.8s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.7s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   4.0s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.8s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.7s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   2.7s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   3.2s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   4.0s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.7s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.8s\n",
      "[CV] END feature_selection__percentile=99, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TreebankWordTokenizer, vectorization__max_df=0.99, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   2.8s\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   1.1s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   1.0s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   1.0s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   1.0s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   1.0s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   1.0s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   1.0s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   1.0s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=   1.0s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   4.9s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   3.5s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   3.2s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   4.2s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=count; total time=   4.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   3.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   3.8s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   5.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   4.1s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 3), vectorization__type=tfidf; total time=   3.7s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   0.9s\n",
      "[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, lemmatization__pos=v, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.5, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=count; total time=   1.4s\n"
     ]
    }
   ],
   "source": [
    "# MODEL COMPARISON\n",
    "df_train, size, class_dictionary = load_dataset('colab',0.1,change_labels=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train['message'], df_train['sentiment'], test_size=0.2, random_state=42, stratify=df_train['sentiment'])\n",
    "preprocessing_steps = [\n",
    "                       ('noise_removal', NoiseRemover()),\n",
    "                       ('emoticon_convertion', EmoticonConverter()),\n",
    "                       ('punctuation_removal', PunctuationRemover()),\n",
    "                       ('tokenization', Tokenizer()),\n",
    "                       ('garbageout', GarbageRemover()),\n",
    "                       ('stopword_removal', StopwordRemover()),\n",
    "                       ('lemmatization', Lemmatizer()),\n",
    "                       ('vectorization', Vectorize()),\n",
    "                       #('oversampler', RandomOverSampler(sampling_strategy=oversampling_strategy, random_state=42)),\n",
    "                       #('undersampler', RandomUnderSampler(sampling_strategy=undersampling_strategy, random_state=42)),\n",
    "                       ('smote', SMOTE()),\n",
    "                       ('scaler', Scaler()),\n",
    "                       ('feature_selection', FeatureSelector())\n",
    "                       ]\n",
    "models =        {\n",
    "                 'Multinomial Naive Bayes': MultinomialNB(),\n",
    "                 'Logistic Regression': LogisticRegression(),\n",
    "                 #'KNN': KNeighborsClassifier(n_neighbors=3, algorithm='ball_tree', leaf_size=30, metric='minkowski', p=2, weights='distance'),\n",
    "                 #'SVC - linear': SVC(kernel=\"linear\", C=0.025),\n",
    "                 #'SVC - RBF': SVC(gamma=1, C=1),\n",
    "                 #'DecisionTree': DecisionTreeClassifier(max_depth=5),\n",
    "                 #'RandomForest': RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1, random_state=42),\n",
    "                 #'AdaBoost': AdaBoostClassifier(random_state=42, n_estimators=200),\n",
    "                }\n",
    "\n",
    "# Create the pipelines\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipeline = Pipeline(preprocessing_steps  + [('model', model)])\n",
    "    param_grid = {\n",
    "    'tokenization__type': ['TweetTokenizer','TreebankWordTokenizer'],\n",
    "    'lemmatization__pos': ['v'],#,'n'],#,'v','r'],\n",
    "    'vectorization__type': ['count','tfidf'],\n",
    "    'vectorization__max_df': [0.25,0.5,0.99],\n",
    "    'vectorization__min_df': [1],#, 2],\n",
    "    'vectorization__ngram_range': [(1,1),(1,3)],# (1, 10)],\n",
    "    'vectorization__max_features': [None],\n",
    "    'scaler__type': ['robust'],  # 'minmax','maxabs'],\n",
    "    'smote__k_neighbors': [3],\n",
    "    'smote__random_state': [42],\n",
    "    'feature_selection__percentile': [25,50,99],\n",
    "    'feature_selection__type': ['anova_f'],\n",
    "                }\n",
    "\n",
    "    # Create GridSearchCV object\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1_macro', error_score='raise', verbose=2)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    df_gridsearch = pd.DataFrame(grid_search.cv_results_)\n",
    "    #Create blank results dataframe - only do this once\n",
    "    selected_columns = ['Model','Dataset','Timestamp','mean_fit_time','mean_score_time','params','split0_test_score','split1_test_score','split2_test_score','split3_test_score','split4_test_score','std_test_score','mean_test_score']\n",
    "    #results_df = pd.DataFrame(columns=selected_columns)\n",
    "    #results_df.to_csv('/content/drive/MyDrive/Professionele ontwikkeling/Data Science/Explore Data Science Course/Sprint 6_Advanced Classification/Predict/advanced-classification-predict/notebook/Model_Comparison results.csv', index=False)\n",
    "    new_data = pd.DataFrame(columns=selected_columns)\n",
    "    new_data[['mean_fit_time','mean_score_time','params','split0_test_score','split1_test_score',\n",
    "         'split2_test_score','split3_test_score','split4_test_score','std_test_score','mean_test_score']] = df_gridsearch[['mean_fit_time',\n",
    "         'mean_score_time','params','split0_test_score','split1_test_score','split2_test_score','split3_test_score','split4_test_score','std_test_score','mean_test_score']]\n",
    "    new_data['Model'] = name\n",
    "    new_data['Dataset'] = size\n",
    "    new_data['Timestamp']= datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    new_data.to_csv('/content/drive/MyDrive/Professionele ontwikkeling/Data Science/Explore Data Science Course/Sprint 6_Advanced Classification/Predict/advanced-classification-predict/notebook/Model_Comparison results.csv', mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5fuiWQY24err"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Evaluate the pipeline\n",
    "accuracy = pipeline.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "model_stats = pipeline.named_steps['model']\n",
    "\n",
    "# Obtain the number of features used by the model\n",
    "attributes = model_stats.__dict__\n",
    "\n",
    "# Print all attributes\n",
    "for attr, value in attributes.items():\n",
    "    print(attr, \":\", value)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FaIL1W_gpHrG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQoIB6k0XiRF"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f'G:/MyDrive/Professionele ontwikkeling/Data Science/Explore Data Science Course/Sprint 6_Advanced Classification/Predict/advanced-classification-predict/notebook/dawieloots_predict_gridsearch_{timestamp}.csv'\n",
    "df_gridsearch.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_lxVdk5DJ4Ci"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(df_gridsearch)\n",
    "print(grid_search.get_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6fbMy5RktY2"
   },
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Custom transformer to wrap SMOTE\n",
    "\n",
    "class ResampleAndFeatureSelectTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, k=1000, score_func=chi2, k_neighbors=5):\n",
    "        self.k = k\n",
    "        self.score_func = score_func\n",
    "        self.k_neighbors = k_neighbors\n",
    "        self.feature_selector = SelectKBest(score_func=self.score_func, k=self.k)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Print the shape of X_train before preprocessing\n",
    "               \n",
    "        # Select features from the resampled data\n",
    "        self.feature_selector.fit(X, y)\n",
    "        \n",
    "        # Print the shape of X_train after preprocessing\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Select features from the input data\n",
    "        X_selected = self.feature_selector.transform(X)\n",
    "        return X_selected\n",
    "\n",
    "# Load the 20 Newsgroups dataset\n",
    "data = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "data_subset = {\n",
    "    'data': data.data[:1000],\n",
    "    'target': data.target[:1000],\n",
    "    'target_names': data.target_names\n",
    "}\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_subset['data'], data_subset['target'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Define preprocessing steps\n",
    "preprocessing_steps = [\n",
    "    # Text preprocessing\n",
    "    ('vectorizer', TfidfVectorizer()),  # Convert text data into numerical vectors\n",
    "    ('smote', SMOTE()),\n",
    "    ('resample_and_feature_select', ResampleAndFeatureSelectTransformer(k=1000, score_func=chi2)),  # Resample and select top k features\n",
    "    \n",
    "]\n",
    "\n",
    "# Define the model\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(preprocessing_steps + [('model', model)])\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the pipeline\n",
    "accuracy = pipeline.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "model = pipeline.named_steps['model']\n",
    "\n",
    "# Obtain the number of features used by the model\n",
    "attributes = model.__dict__\n",
    "\n",
    "# Print all attributes\n",
    "for attr, value in attributes.items():\n",
    "    print(attr, \":\", value)\n",
    "#n_features = model.n_features_\n",
    "#print(\"Number of features used by the model:\", n_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491
    },
    "executionInfo": {
     "elapsed": 25390,
     "status": "ok",
     "timestamp": 1712856834355,
     "user": {
      "displayName": "Dawie Loots",
      "userId": "00798578190767538914"
     },
     "user_tz": -120
    },
    "id": "AyoIXSk4rU58",
    "outputId": "4e3b6821-e68d-4ff7-df70-79e7ea62130e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Dataset original shape: (15819, 3)\n",
      "Dataset original class distribution: {1: 8530, 2: 3640, 0: 2353, -1: 1296}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;noise_removal&#x27;, NoiseRemover()),\n",
       "                (&#x27;emoticon_convertion&#x27;, EmoticonConverter()),\n",
       "                (&#x27;punctuation_removal&#x27;, PunctuationRemover()),\n",
       "                (&#x27;tokenization&#x27;, Tokenizer(type=&#x27;TweetTokenizer&#x27;)),\n",
       "                (&#x27;stopword_removal&#x27;, StopwordRemover()),\n",
       "                (&#x27;lemmatization&#x27;, Lemmatizer()),\n",
       "                (&#x27;vectorization&#x27;, Vectorize(max_df=0.75)),\n",
       "                (&#x27;smote&#x27;, SMOTE(k_neighbors=4, random_state=42)),\n",
       "                (&#x27;scaler&#x27;, Scaler()),\n",
       "                (&#x27;feature_selection&#x27;, FeatureSelector(percentile=99)),\n",
       "                (&#x27;model&#x27;,\n",
       "                 LogisticRegression(C=1, max_iter=1000000,\n",
       "                                    multi_class=&#x27;multinomial&#x27;, random_state=42,\n",
       "                                    solver=&#x27;sag&#x27;, tol=0.001))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;noise_removal&#x27;, NoiseRemover()),\n",
       "                (&#x27;emoticon_convertion&#x27;, EmoticonConverter()),\n",
       "                (&#x27;punctuation_removal&#x27;, PunctuationRemover()),\n",
       "                (&#x27;tokenization&#x27;, Tokenizer(type=&#x27;TweetTokenizer&#x27;)),\n",
       "                (&#x27;stopword_removal&#x27;, StopwordRemover()),\n",
       "                (&#x27;lemmatization&#x27;, Lemmatizer()),\n",
       "                (&#x27;vectorization&#x27;, Vectorize(max_df=0.75)),\n",
       "                (&#x27;smote&#x27;, SMOTE(k_neighbors=4, random_state=42)),\n",
       "                (&#x27;scaler&#x27;, Scaler()),\n",
       "                (&#x27;feature_selection&#x27;, FeatureSelector(percentile=99)),\n",
       "                (&#x27;model&#x27;,\n",
       "                 LogisticRegression(C=1, max_iter=1000000,\n",
       "                                    multi_class=&#x27;multinomial&#x27;, random_state=42,\n",
       "                                    solver=&#x27;sag&#x27;, tol=0.001))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NoiseRemover</label><div class=\"sk-toggleable__content\"><pre>NoiseRemover()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">EmoticonConverter</label><div class=\"sk-toggleable__content\"><pre>EmoticonConverter()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PunctuationRemover</label><div class=\"sk-toggleable__content\"><pre>PunctuationRemover()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Tokenizer</label><div class=\"sk-toggleable__content\"><pre>Tokenizer(type=&#x27;TweetTokenizer&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StopwordRemover</label><div class=\"sk-toggleable__content\"><pre>StopwordRemover()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Lemmatizer</label><div class=\"sk-toggleable__content\"><pre>Lemmatizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Vectorize</label><div class=\"sk-toggleable__content\"><pre>Vectorize(max_df=0.75)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SMOTE</label><div class=\"sk-toggleable__content\"><pre>SMOTE(k_neighbors=4, random_state=42)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Scaler</label><div class=\"sk-toggleable__content\"><pre>Scaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FeatureSelector</label><div class=\"sk-toggleable__content\"><pre>FeatureSelector(percentile=99)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=1, max_iter=1000000, multi_class=&#x27;multinomial&#x27;,\n",
       "                   random_state=42, solver=&#x27;sag&#x27;, tol=0.001)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('noise_removal', NoiseRemover()),\n",
       "                ('emoticon_convertion', EmoticonConverter()),\n",
       "                ('punctuation_removal', PunctuationRemover()),\n",
       "                ('tokenization', Tokenizer(type='TweetTokenizer')),\n",
       "                ('stopword_removal', StopwordRemover()),\n",
       "                ('lemmatization', Lemmatizer()),\n",
       "                ('vectorization', Vectorize(max_df=0.75)),\n",
       "                ('smote', SMOTE(k_neighbors=4, random_state=42)),\n",
       "                ('scaler', Scaler()),\n",
       "                ('feature_selection', FeatureSelector(percentile=99)),\n",
       "                ('model',\n",
       "                 LogisticRegression(C=1, max_iter=1000000,\n",
       "                                    multi_class='multinomial', random_state=42,\n",
       "                                    solver='sag', tol=0.001))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Final model building with full trainset\n",
    "df_train, _, _ = load_dataset('colab',1)\n",
    "#df_train['sentiment'] = transform_categorical_labels(df_train['sentiment'])\n",
    "X_train = df_train['message']\n",
    "y_train = df_train['sentiment']\n",
    "\n",
    "preprocessing_steps = [\n",
    "                       ('noise_removal', NoiseRemover()),\n",
    "                       ('emoticon_convertion', EmoticonConverter()),\n",
    "                       ('punctuation_removal', PunctuationRemover()),\n",
    "                       ('tokenization', Tokenizer(type='TweetTokenizer')),\n",
    "                       ('stopword_removal', StopwordRemover()),\n",
    "                       ('lemmatization', Lemmatizer()),\n",
    "                       ('vectorization', Vectorize(max_df=0.75, min_df=1, ngram_range=(1,1),max_features=None, type='tfidf')),\n",
    "                       ('smote', SMOTE(k_neighbors=4, random_state=42)),\n",
    "                       ('scaler', Scaler(type='robust')),\n",
    "                       ('feature_selection', FeatureSelector(percentile=99, type='anova_f'))\n",
    "                       ]\n",
    "\n",
    "#model = SVC(C=10,gamma=0.01)  #THIS SEEMS TO BE THE BEST\n",
    "model = LogisticRegression(C=1, max_iter=1000000,multi_class='multinomial', solver='sag', tol=0.001, penalty='l2', random_state=42)\n",
    "pipeline = Pipeline(preprocessing_steps  + [('model', model)])\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1712856834359,
     "user": {
      "displayName": "Dawie Loots",
      "userId": "00798578190767538914"
     },
     "user_tz": -120
    },
    "id": "rkdRn-om_QiA",
    "outputId": "0f009148-eae8-4f39-bae7-553b243ce5c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "penalty : l2\n",
      "dual : False\n",
      "tol : 0.001\n",
      "C : 1\n",
      "fit_intercept : True\n",
      "intercept_scaling : 1\n",
      "class_weight : None\n",
      "random_state : 42\n",
      "solver : sag\n",
      "max_iter : 1000000\n",
      "multi_class : multinomial\n",
      "verbose : 0\n",
      "warm_start : False\n",
      "n_jobs : None\n",
      "l1_ratio : None\n",
      "n_features_in_ : 22481\n",
      "classes_ : [-1  0  1  2]\n",
      "n_iter_ : [55]\n",
      "coef_ : [[-0.09849407 -0.06170661 -0.0242912  ... -0.0316989  -0.0316989\n",
      "  -0.04595831]\n",
      " [ 0.47132916 -0.01673616 -0.0268516  ...  0.17909438  0.17909438\n",
      "   0.18485926]\n",
      " [-0.11636423  0.17776388  0.06536473 ... -0.0932192  -0.0932192\n",
      "  -0.0767753 ]\n",
      " [-0.25647085 -0.09932111 -0.01422193 ... -0.05417628 -0.05417628\n",
      "  -0.06212565]]\n",
      "intercept_ : [-0.73788254  1.18096025 -0.08638792 -0.35668979]\n"
     ]
    }
   ],
   "source": [
    "model_stats = pipeline.named_steps['model']\n",
    "\n",
    "# Obtain the number of features used by the model\n",
    "attributes = model_stats.__dict__\n",
    "\n",
    "# Print all attributes\n",
    "for attr, value in attributes.items():\n",
    "    print(attr, \":\", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T6DW763d3M--"
   },
   "outputs": [],
   "source": [
    "test_file = '/content/drive/MyDrive/Professionele ontwikkeling/Data Science/Explore Data Science Course/Sprint 6_Advanced Classification/Predict/advanced-classification-predict/data/test_with_no_labels.csv'\n",
    "df_test = pd.read_csv(test_file)\n",
    "X_unseen = df_test['message']\n",
    "\n",
    "# Predict\n",
    "y_pred = pipeline.predict(X_unseen)\n",
    "label_map = {\n",
    "        'News': 2,\n",
    "        'Pro': 1,\n",
    "        'Neutral': 0,\n",
    "        'Anti': -1\n",
    "    }\n",
    "#y_pred_int = [label_map[label] for label in y_pred_text]\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f'/content/drive/MyDrive/Professionele ontwikkeling/Data Science/Explore Data Science Course/Sprint 6_Advanced Classification/Predict/advanced-classification-predict/notebook/dawieloots_predict_{timestamp}.csv'\n",
    "submission_df = pd.DataFrame()\n",
    "submission_df['tweetid'] = df_test.tweetid\n",
    "submission_df['sentiment'] = y_pred\n",
    "submission_df.to_csv(filename, index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1OxjFEie5eThMMpfvLVib244h1m4KVxzQ",
     "timestamp": 1713001851986
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

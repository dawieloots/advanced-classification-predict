{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 601,
     "status": "ok",
     "timestamp": 1712996860193,
     "user": {
      "displayName": "Dawie Loots",
      "userId": "00798578190767538914"
     },
     "user_tz": -120
    },
    "id": "h20rlqi05cIw",
    "outputId": "6703fe83-3a4a-4dba-c61b-8a4ada2ae66b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dawie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dawie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Libraries for data loading, data manipulation and data visulisation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import chardet # To provide a best estimate of the encoding that was used in the text data\n",
    "import io # For string operations\n",
    "%matplotlib inline\n",
    "\n",
    "# Libraries for data preparation and model building\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer, TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import datetime\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import math\n",
    "import re\n",
    "from sklearn.utils import resample\n",
    "from scipy.sparse import vstack\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import feature_selection\n",
    "from sklearn.feature_selection import SelectPercentile, SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score, log_loss\n",
    "from sklearn.model_selection import GridSearchCV, KFold, StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE,SMOTENC\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2hMYSMSB6fHp"
   },
   "outputs": [],
   "source": [
    "# LOAD DATASET\n",
    "\n",
    "def transform_categorical_labels(data):\n",
    "    # Dictionary mapping numerical categories to labels\n",
    "    label_map = {\n",
    "        2: 'News',\n",
    "        1: 'Pro',\n",
    "        0: 'Neutral',\n",
    "        -1: 'Anti'\n",
    "    }\n",
    "    transformed_data = data.map(label_map)\n",
    "    return transformed_data\n",
    "\n",
    "def class_distribution(data):\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        unique_classes, class_counts = data.iloc[:, 0].value_counts().index, data.iloc[:, 0].value_counts().values\n",
    "    elif isinstance(data, pd.Series):\n",
    "        unique_classes, class_counts = data.value_counts().index, data.value_counts().values\n",
    "    class_dict = dict(zip(unique_classes, class_counts))\n",
    "    return class_dict\n",
    "\n",
    "def load_dataset(environment, size, change_labels):\n",
    "    if environment == 'colab':\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        csv_file = '/content/drive/MyDrive/Professionele ontwikkeling/Data Science/Explore Data Science Course/Sprint 6_Advanced Classification/Predict/advanced-classification-predict/data/train.csv'\n",
    "    else:\n",
    "        csv_file = r'G:\\My Drive\\Professionele ontwikkeling\\Data Science\\Explore Data Science Course\\Sprint 6_Advanced Classification\\Predict\\advanced-classification-predict\\data\\train.csv'\n",
    "    df = pd.read_csv(csv_file)\n",
    "    if change_labels:\n",
    "        df['sentiment'] = transform_categorical_labels(df['sentiment'])\n",
    "    sample_size = int(len(df) * size)\n",
    "    if size == 1:\n",
    "        pass\n",
    "    else:\n",
    "        X = df.drop(columns=['sentiment']).copy()\n",
    "        y = df.sentiment.copy()\n",
    "        X_sample, _, y_sample, _ = train_test_split(X, y, train_size=sample_size, stratify=y, random_state=42)\n",
    "        df = pd.concat([X_sample, y_sample], axis=1)\n",
    "\n",
    "#       np.random.seed(42)  # OLD CODE\n",
    "#       pd.read_csv(csv_file).shape[0] - n  # OLD CODE\n",
    "#        df = pd.read_csv(csv_file, skiprows=lambda i: i > 0 and np.random.rand() > n / (i + 1), nrows=n)  # OLD CODE\n",
    "\n",
    "    class_dict = class_distribution(df['sentiment'])\n",
    "\n",
    "    return df, sample_size, class_dict\n",
    "\n",
    "\n",
    "# def load_full_dataset():  # OLD CODE\n",
    "#   df = pd.read_csv(csv_file)  # OLD CODE\n",
    "#   return df, 'full'  # OLD CODE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUVu-k0BXiQ3"
   },
   "source": [
    "df_train = pd.read_csv('G:/MyDrive/Professionele ontwikkeling/Data Science/Explore Data Science Course/Sprint 6_Advanced Classification/Predict/advanced-classification-predict/data/train.csv')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1rI10jOolRbZ"
   },
   "outputs": [],
   "source": [
    "class NoiseRemover(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        pattern_url = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'   # Find all hyperlinks\n",
    "        subs_url = r''\n",
    "        X_transformed = pd.Series(X).replace(to_replace = pattern_url, value = subs_url, regex = True)\n",
    "        return X_transformed\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "npvvupgdqXbC"
   },
   "outputs": [],
   "source": [
    "class EmoticonConverter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        emoticon_dictionary = {':\\)': 'smiley_face_emoticon',\n",
    "                               ':\\(': 'frowning_face_emoticon',\n",
    "                               ':D': 'grinning_face_emoticon',\n",
    "                               ':P': 'sticking_out_tongue_emoticon',\n",
    "                               ';\\)': 'winking_face_emoticon',\n",
    "                               ':o': 'surprised_face_emoticon',\n",
    "                               ':\\|': 'neutral_face_emoticon',\n",
    "                               ':\\'\\)': 'tears_of_joy_emoticon',\n",
    "                               ':\\'\\(': 'crying_face_emoticon'}\n",
    "        X_transformed = X.replace(emoticon_dictionary, regex=True)\n",
    "        return X_transformed\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "VTRod0QUsOL7"
   },
   "outputs": [],
   "source": [
    "class PunctuationRemover(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        def expand_contractions(text):\n",
    "            contractions = {\"'t\": \" not\",\"'s\": \" is\",\"'re\": \" are\",\"'ll\": \" will\", \"'m\": \" am\"}\n",
    "            pattern = re.compile(r\"\\b(\" + \"|\".join(re.escape(key) for key in contractions.keys()) + r\")\\b\")\n",
    "            text = re.sub(r\"n't\\b\", \" not\", text) # Replace \"n't\" with \" not\"\n",
    "            text = pattern.sub(lambda match: contractions[match.group(0)], text) # Replace all other contractions except for \"n't\"\n",
    "            return text\n",
    "\n",
    "        def remove_punctuation(text):\n",
    "            return ''.join([l.lower() for l in text if l not in string.punctuation])\n",
    "\n",
    "        X_transformed = X.apply(lambda x: expand_contractions(x))\n",
    "        X_transformed = X_transformed.apply(lambda x: remove_punctuation(x))\n",
    "        return X_transformed\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "b7cme7kUuyIt"
   },
   "outputs": [],
   "source": [
    "class Tokenizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, type=None):\n",
    "        self.type = type\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if self.type == 'TweetTokenizer':\n",
    "            tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True)\n",
    "        elif self.type == 'TreebankWordTokenizer':\n",
    "            tokenizer = TreebankWordTokenizer()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid tokenizer type. Choose either 'TweetTokenizer' or 'TreebankWordTokenizer'\")\n",
    "        X_transformed = X.apply(tokenizer.tokenize)\n",
    "        return X_transformed\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "rKXIUNy-TF-b"
   },
   "outputs": [],
   "source": [
    "class GarbageRemover(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, type=None):\n",
    "        self.type = type\n",
    "        self.pattern = re.compile(r'^[a-zA-Z\\U0001F300-\\U0001FAD6\\U0001F004-\\U0001F251]*[^a-zA-Z\\U0001F300-\\U0001FAD6\\U0001F004-\\U0001F251]+[a-zA-Z\\U0001F300-\\U0001FAD6\\U0001F004-\\U0001F251]*$')\n",
    "    def is_garbage(self, word):\n",
    "        \"\"\"\n",
    "        Define a regular expression pattern to match garbage words\n",
    "        Garbage is defined as words starting with anything but a letter, or\n",
    "        words containing strange characters.  Emojis are not considered garbage\n",
    "        \"\"\"\n",
    "        return bool(self.pattern.match(word))\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        transformed_words = [[word for word in word_list if not self.is_garbage(word)] or ['blankpost'] for word_list in X]\n",
    "        return pd.Series(transformed_words)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-puz8STKxpt3"
   },
   "outputs": [],
   "source": [
    "class StopwordRemover(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = X.apply(lambda tokens: [t for t in tokens if t.lower() not in self.stop_words])\n",
    "        return X_transformed\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1miNSgYK4zfz"
   },
   "outputs": [],
   "source": [
    "class Lemmatizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, pos=None):\n",
    "        self.pos = pos\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if self.pos not in ['v','n', 'a', 'r']:\n",
    "            raise ValueError('Invalid pos type. Choose either a, n, r or v')\n",
    "        else:\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            X_transformed = X.apply(lambda tokens: [lemmatizer.lemmatize(word, pos=self.pos) for word in tokens])\n",
    "        return X_transformed\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "XYw7dmak74hm"
   },
   "outputs": [],
   "source": [
    "class Vectorize(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, type=None, max_df=None, min_df=None, ngram_range=None, max_features=None):\n",
    "        self.type = type\n",
    "        self.max_df = max_df\n",
    "        self.min_df = min_df\n",
    "        self.ngram_range = ngram_range\n",
    "        self.max_features = max_features\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.type == 'count':\n",
    "            self.vectorizer = CountVectorizer(max_features=self.max_features, lowercase=True,\n",
    "                                              max_df=self.max_df, min_df=self.min_df,\n",
    "                                              ngram_range=self.ngram_range)\n",
    "        elif self.type == 'tfidf':\n",
    "            self.vectorizer = TfidfVectorizer(max_features=self.max_features, lowercase=True,\n",
    "                                              max_df=self.max_df, min_df=self.min_df,\n",
    "                                              ngram_range=self.ngram_range)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid vectorizer type. Choose either 'count' or 'tfidf'\")\n",
    "\n",
    "        X_joined = [' '.join(tokens) for tokens in X]\n",
    "        self.vectorizer.fit(X_joined)\n",
    "        return self.vectorizer.fit(X_joined)\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_joined = [' '.join(tokens) for tokens in X]\n",
    "        return self.vectorizer.transform(X_joined)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "TP0o7OpfGpg3"
   },
   "outputs": [],
   "source": [
    "class Scaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, type=None):\n",
    "        self.type = type\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.type == 'robust':\n",
    "            self.scaler = RobustScaler(with_centering=False)\n",
    "        elif self.type == 'minmax':\n",
    "            self.scaler = MinMaxScaler(with_centering=False)\n",
    "        elif self.type == 'maxabs':\n",
    "            self.scaler = MaxAbsScaler(with_centering=False)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid scaler type. Choose between 'robust', 'minmax' or 'maxabs'.\")\n",
    "        self.scaler.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return self.scaler.transform(X)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "R5UyERuHxKKI"
   },
   "outputs": [],
   "source": [
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, type=None, percentile=None):\n",
    "        self.type = type\n",
    "        self.percentile = percentile\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.type == 'mutualinfo':\n",
    "            self.selector = SelectPercentile(score_func=mutual_info_classif, percentile=self.percentile)\n",
    "        elif self.type == 'anova_f':\n",
    "            self.selector = SelectPercentile(score_func=f_classif, percentile=self.percentile)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid selector type. Choose between 'mutualinfo' or 'anova_f'.\")\n",
    "        self.selector.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return self.selector.transform(X)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhW2csKbHUnB"
   },
   "source": [
    "X = df_train['message']\n",
    "y = df_train['sentiment']\n",
    "noise = NoiseRemover()\n",
    "X = noise.fit_transform(X)\n",
    "emoti = EmoticonConverter()\n",
    "X = emoti.fit_transform(X)\n",
    "punc = PunctuationRemover()\n",
    "X = punc.fit_transform(X)\n",
    "token = Tokenizer(type='TweetTokenizer')\n",
    "X = token.fit_transform(X)\n",
    "garbage = GarbageRemover()\n",
    "X = garbage.fit_transform(X)\n",
    "stop = StopwordRemover()\n",
    "X = stop.fit_transform(X)\n",
    "print(X)\n",
    "\n",
    "\"\"\"\n",
    "lemma = Lemmatizer(pos='v')\n",
    "X = lemma.fit_transform(X)\n",
    "\n",
    "words_list = X.explode().tolist()\n",
    "\n",
    "\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_word_cloud(words, title):\n",
    "    # Join the words into a single string\n",
    "    text = ' '.join(words)\n",
    "\n",
    "    # Generate a word cloud\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "\n",
    "    # Plot the word cloud\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "create_word_cloud(words_list, \"All Words\")\n",
    "\n",
    "def classify_words(words):\n",
    "    actual_words = []\n",
    "    garbage_words = []\n",
    "\n",
    "    # Regular expression pattern to match a word\n",
    "    word_pattern = re.compile(r'^[a-zA-Z]+$')\n",
    "\n",
    "    for word in words:\n",
    "        # Check if the word matches the pattern\n",
    "        if word_pattern.match(word) and word:\n",
    "            actual_words.append(word)\n",
    "        else:\n",
    "            garbage_words.append(word)\n",
    "\n",
    "    return actual_words, garbage_words\n",
    "\n",
    "# Example usage:\n",
    "actual, garbage = classify_words(words_list)\n",
    "print(f'Actual words: {len(actual)}')\n",
    "print(f'Garbage words: {len(garbage)}')\n",
    "\n",
    "create_word_cloud(actual, \"Actual Words\")\n",
    "create_word_cloud(garbage, \"Garbage\")\n",
    "\n",
    "import csv\n",
    "\n",
    "\n",
    "def is_garbage(word):\n",
    "    # Define a regular expression pattern to match garbage words\n",
    "    pattern = r'^[a-zA-Z\\U0001F300-\\U0001FAD6\\U0001F004-\\U0001F251]*[^a-zA-Z\\U0001F300-\\U0001FAD6\\U0001F004-\\U0001F251]+[a-zA-Z\\U0001F300-\\U0001FAD6\\U0001F004-\\U0001F251]*$'\n",
    "    # Check if the word matches the pattern\n",
    "    return bool(re.match(pattern, word))\n",
    "\n",
    "def count_garbage_words(words):\n",
    "    garbage_count = {}\n",
    "\n",
    "    for word in words:\n",
    "        if is_garbage(word):\n",
    "            if word in garbage_count:\n",
    "                garbage_count[word] += 1\n",
    "            else:\n",
    "                garbage_count[word] = 1\n",
    "\n",
    "    return garbage_count\n",
    "\n",
    "def export_to_csv(garbage_counts, filename):\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['Garbage Word', 'Count']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        writer.writeheader()\n",
    "        for word, count in garbage_counts.items():\n",
    "            writer.writerow({'Garbage Word': word, 'Count': count})\n",
    "\n",
    "# Example usage:\n",
    "garbage_counts = count_garbage_words(words_list)\n",
    "\n",
    "# Export to CSV\n",
    "export_to_csv(garbage_counts, '/content/drive/MyDrive/Professionele ontwikkeling/Data Science/Explore Data Science Course/Sprint 6_Advanced Classification/Predict/advanced-classification-predict/notebook/garbagewords.csv')\n",
    "\n",
    "\n",
    "def words_summary(words):\n",
    "    summary = [0] * (max(len(word) for word in words) + 1)\n",
    "\n",
    "    # Iterate through the words\n",
    "    for word in words:\n",
    "        # Get the length of the word\n",
    "        length = len(word)\n",
    "        # Increment the count for the corresponding length\n",
    "        summary[length] += 1\n",
    "\n",
    "    return summary\n",
    "\n",
    "# Example usage:\n",
    "summary = words_summary(words_list)\n",
    "\n",
    "# Print the summary\n",
    "print(\"Summary of words per length:\")\n",
    "for length, count in enumerate(summary):\n",
    "    if count > 0:\n",
    "        print(f\"Length {length}: {count} words\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "executionInfo": {
     "elapsed": 40077,
     "status": "ok",
     "timestamp": 1712996901007,
     "user": {
      "displayName": "Dawie Loots",
      "userId": "00798578190767538914"
     },
     "user_tz": -120
    },
    "id": "QkpIKZ6Nfd7M",
    "outputId": "7a8df809-cdc2-4492-d0c6-0ea00806ae8b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "3pDtr9uCNIfh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34871,
     "status": "ok",
     "timestamp": 1713000518414,
     "user": {
      "displayName": "Dawie Loots",
      "userId": "00798578190767538914"
     },
     "user_tz": -120
    },
    "id": "5fVqF62W5XKf",
    "outputId": "f6681028-65ba-4ca9-ab28-1e76aa19f341"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (2530,)\n",
      "Shape of y_train: (2530,)\n",
      "Shape of X_test: (633,)\n",
      "Shape of y_test: (633,)\n",
      "Distribution of y_train: {2: 1365, 3: 582, 1: 376, 0: 207}\n",
      "Distribution of y_test: {2: 341, 3: 146, 1: 94, 0: 52}\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END feature_selection__percentile=100, feature_selection__type=anova_f, lemmatization__pos=v, model__learning_rate=0.3, model__max_depth=30, model__n_estimators=100, model__subsample=1.0, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.75, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time= 1.4min\n",
      "[CV] END feature_selection__percentile=100, feature_selection__type=anova_f, lemmatization__pos=v, model__learning_rate=0.3, model__max_depth=30, model__n_estimators=100, model__subsample=1.0, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.75, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time= 1.4min\n",
      "[CV] END feature_selection__percentile=100, feature_selection__type=anova_f, lemmatization__pos=v, model__learning_rate=0.3, model__max_depth=30, model__n_estimators=100, model__subsample=1.0, scaler__type=robust, smote__k_neighbors=3, smote__random_state=42, tokenization__type=TweetTokenizer, vectorization__max_df=0.75, vectorization__max_features=None, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time= 1.3min\n"
     ]
    }
   ],
   "source": [
    "label_change = True\n",
    "df_train, size, class_dictionary = load_dataset('jupyter',0.2,label_change)\n",
    "# Next section is to calculate class imbalance, for us in randomover and randomundersampling\n",
    "## NEXT 3 LINES FOR XGBOOST ONLY\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "df_train[\"sentiment\"] = label_encoder.fit_transform(df_train[\"sentiment\"])\n",
    "\n",
    "number_classes = len(class_dictionary)\n",
    "majority = max(class_dictionary.values())\n",
    "average_class_count = int(size / number_classes)\n",
    "oversampling_strategy = {key: max(average_class_count, count)  for key, count in class_dictionary.items()}\n",
    "undersampling_strategy = {key: average_class_count  for key, count in oversampling_strategy.items()}\n",
    "\n",
    "df_train.head()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train['message'], df_train['sentiment'], test_size=0.2, random_state=42, stratify=df_train['sentiment'])\n",
    "print(f'Shape of X_train: {X_train.shape}')\n",
    "print(f'Shape of y_train: {y_train.shape}')\n",
    "print(f'Shape of X_test: {X_test.shape}')\n",
    "print(f'Shape of y_test: {y_test.shape}')\n",
    "print(f'Distribution of y_train: {class_distribution(y_train)}')\n",
    "print(f'Distribution of y_test: {class_distribution(y_test)}')\n",
    "\n",
    "# Defining preprocessing steps\n",
    "preprocessing_steps = [\n",
    "\n",
    "                       ('noise_removal', NoiseRemover()),\n",
    "                       ('emoticon_convertion', EmoticonConverter()),\n",
    "                       ('punctuation_removal', PunctuationRemover()),\n",
    "                       ('tokenization', Tokenizer()),\n",
    "                       ('garbageout', GarbageRemover()),\n",
    "                       ('stopword_removal', StopwordRemover()),\n",
    "                       ('lemmatization', Lemmatizer()),\n",
    "                       ('vectorization', Vectorize()),\n",
    "                       #('oversampler', RandomOverSampler(sampling_strategy=oversampling_strategy, random_state=42)),\n",
    "                       #('undersampler', RandomUnderSampler(sampling_strategy=undersampling_strategy, random_state=42)),\n",
    "                       ('smote', SMOTE()),\n",
    "                       ('scaler', Scaler()),\n",
    "                       ('feature_selection', FeatureSelector())\n",
    "                       ]\n",
    "\n",
    "\n",
    "# Create the pipelines\n",
    "#model = xgb.XGBClassifier()\n",
    "#model = AdaBoostClassifier()\n",
    "#model_name = 'Adaboost'\n",
    "#model = MultinomialNB()\n",
    "#model_name = 'MultinomialNB'\n",
    "#model = LogisticRegression()\n",
    "#model_name = 'LogisticRegression'\n",
    "#model = KNeighborsClassifier()\n",
    "#model_name = 'KNN'\n",
    "#model = SVC(C=100,gamma=0.1)  #THIS SEEMS TO BE THE BEST\n",
    "#model = SVC()\n",
    "#model_name='SVC RBF'\n",
    "#model = RandomForestClassifier()\n",
    "#model_name='RandomForest'\n",
    "model = xgb.XGBClassifier()\n",
    "model_name='XGBoost'\n",
    "#model = MLPClassifier(hidden_layer_sizes=(100, 50), activation='relu', solver='adam', alpha=0.0001,\n",
    "#                    batch_size='auto', learning_rate='constant', learning_rate_init=0.001,\n",
    "#                    max_iter=200, random_state=42)\n",
    "\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense\n",
    "#from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "#from keras.utils import to_categorical\n",
    "\n",
    "#def create_model():\n",
    "#    model = Sequential()\n",
    "#    model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "#    model.add(Dense(3, activation='softmax'))  # 3 classes, change as per your number of classes\n",
    "#    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#    return model\n",
    "\n",
    "\n",
    "# Wrap Keras model in a scikit-learn compatible classifier\n",
    "#model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=10, verbose=0)\n",
    "#label_encoder = LabelEncoder()\n",
    "#y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "#y_test_encoded = label_encoder.transform(y_test)\n",
    "# Convert integers to one-hot encoded format\n",
    "#y_train_one_hot = to_categorical(y_train_encoded)\n",
    "#y_test_one_hot = to_categorical(y_test_encoded)\n",
    "#y_train = y_train_one_hot\n",
    "#yest = y_test_one_hot\n",
    "\n",
    "pipeline = Pipeline(preprocessing_steps  + [('model', model)])\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'tokenization__type': ['TweetTokenizer'],#'TreebankWordTokenizer',],\n",
    "    'lemmatization__pos': ['v'],#,'n'],#,'v','r'],\n",
    "    'vectorization__type': ['tfidf'],#'tfidf'],\n",
    "    'vectorization__max_df': [0.75],#, 0.5],#, 1.0],#,0.5],\n",
    "    'vectorization__min_df': [1],#, 2],\n",
    "    'vectorization__ngram_range': [(1,1)],# (1, 10)],\n",
    "    'vectorization__max_features': [None],\n",
    "    'scaler__type': ['robust'],  # 'minmax','maxabs'],\n",
    "    'smote__k_neighbors': [3],\n",
    "    'smote__random_state': [42],\n",
    "    'feature_selection__percentile': [100],\n",
    "    'feature_selection__type': ['anova_f'],\n",
    "\n",
    "\n",
    "             # Hyperparameters for the model\n",
    "\n",
    "              #'model__n_neighbors': [3,4,5],                                #KNN\n",
    "              #'model__weights': [ 'distance','uniform'],               #KNN\n",
    "              #'model__metric': ['euclidean', 'manhattan'],            #KNN\n",
    "              #'model__C': [100],#10,1],                                  #SVM RBF\n",
    "              #'model__gamma': [0.1],#,0.001, 0.01,1],# 0.1]                                 #SVM RBF\n",
    "               #'model__penalty': ['l2'],#,'l1'],                            #LogisticRegression\n",
    "               #'model__C': [1],#,100,1,0.001,0.1,],                             #LogisticRegression\n",
    "               #'model__solver': ['sag'],#['liblinear']                     #LogisticRegression\n",
    "               #'model__multi_class': ['multinomial'],#'ovr',               #LogisticRegression\n",
    "               #'model__max_iter': [1000000],                               #LogisticRegression\n",
    "               #'model__tol': [0.001],#, 0.0001],                                #LogisticRegression\n",
    "               #'model__n_jobs': [-1],                                           #LogisticRegression\n",
    "               #'model__random_state': [42],                                     #LogisticRegression\n",
    "               #'model__alpha': [0.1],#3, 0.5,],# 0.1,1,5,10],                               #Multinomial NB\n",
    "               #'model__fit_prior': [True],# False],                               #Multinomial NB\n",
    "               #'model__n_estimators': [500],                              #Adaboost\n",
    "               #'model__learning_rate': [0.1],#, 1, 5],                              #Adaboost\n",
    "               #'model__random_state': [42],                                     #Adaboost\n",
    "               #'model__algorithm': ['SAMME'],                                    #Adaboost\n",
    "               #'model__estimator': [LogisticRegression()]#,SVC(kernel='rbf', C=100,gamma=0.1)],DecisionTreeClassifier(), ],   #Adaboost\n",
    "\n",
    "                'model__n_estimators': [100],#200],                               #XGBoost\n",
    "                'model__max_depth': [30],#,6],                               #XGBoost\n",
    "                'model__learning_rate': [0.3],# 1],                               #XGBoost\n",
    "                'model__subsample': [1.0],                               #XGBoost\n",
    "              }\n",
    "\n",
    "# Create GridSearchCV object\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1_macro', error_score='raise', verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "df_gridsearch = pd.DataFrame(grid_search.cv_results_)\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "selected_columns = ['Model','Dataset','Timestamp','mean_fit_time','mean_score_time','label_change','params','split0_test_score','split1_test_score',\n",
    "                    'split2_test_score','split3_test_score','split4_test_score',\n",
    "                    'std_test_score','mean_test_score']\n",
    "\n",
    "#Create blank results dataframe - only do this once\n",
    "###results_df = pd.DataFrame(columns=selected_columns)\n",
    "###results_df.to_csv('/content/drive/MyDrive/Professionele ontwikkeling/Data Science/Explore Data Science Course/Sprint 6_Advanced Classification/Predict/advanced-classification-predict/notebook/Model results.csv', index=False)\n",
    "new_data = pd.DataFrame(columns=selected_columns)\n",
    "\n",
    "new_data[['mean_fit_time','mean_score_time','params','split0_test_score','split1_test_score',\n",
    "         'split2_test_score','split3_test_score','split4_test_score',\n",
    "          'std_test_score','mean_test_score']] = df_gridsearch[['mean_fit_time',\n",
    "         'mean_score_time','params','split0_test_score','split1_test_score',\n",
    "         'split2_test_score','split3_test_score','split4_test_score',\n",
    "         'std_test_score','mean_test_score']]\n",
    "new_data['Model'] = model_name\n",
    "new_data['Dataset'] = size\n",
    "new_data['Timestamp']= datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "new_data['lbael_change'] = label_change\n",
    "\n",
    "new_data.to_csv('G:/My Drive/Professionele ontwikkeling/Data Science/Explore Data Science Course/Sprint 6_Advanced Classification/Predict/advanced-classification-predict/notebook/Model results Jupyter.csv', mode='a', header=False, index=False)\n",
    "\n",
    "\n",
    "# Do a stand-alon pipeline to see the full class report\n",
    "#pipeline.fit(X_train, y_train)\n",
    "#y_pred = pipeline.predict(X_test)\n",
    "#report = classification_report(y_test, y_pred)\n",
    "#print(\"Classification Report for test set:\")\n",
    "#print(report)\n",
    "#print('************************')\n",
    "\n",
    "#k_fold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# Perform k-fold cross-validation and generate predictions\n",
    "#predictions = cross_val_predict(pipeline, X_train, y_train, cv=k_fold)\n",
    "\n",
    "# Generate classification report\n",
    "#report = classification_report(y_train, predictions)\n",
    "\n",
    "# Print the classification report\n",
    "#print(\"Classification Report for CV:\\n\", report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5267,
     "status": "ok",
     "timestamp": 1712749253146,
     "user": {
      "displayName": "Dawie Loots",
      "userId": "00798578190767538914"
     },
     "user_tz": -120
    },
    "id": "K3uO8OX7O3_7",
    "outputId": "e66c80d8-b26a-4695-bd81-7e89ee3f7a3c"
   },
   "outputs": [],
   "source": [
    "scorer = make_scorer(f1_score, average='macro')\n",
    "f1_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring=scorer)\n",
    "print(\"Mean F1 macro score:\", f1_scores.mean())\n",
    "pipeline.fit(X_train, y_train)\n",
    "model_stats = pipeline.named_steps['model']\n",
    "\n",
    "# Obtain the number of features used by the model\n",
    "attributes = model_stats.__dict__\n",
    "\n",
    "# Print all attributes\n",
    "for attr, value in attributes.items():\n",
    "    print(attr, \":\", value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "67oR1ZzJKWCY",
    "outputId": "1b82371e-735c-40a1-d64c-c6a179e9a9b6"
   },
   "outputs": [],
   "source": [
    "# MODEL COMPARISONS\n",
    "\n",
    "# Load the data\n",
    "df_train, size, class_dictionary = load_dataset('colab',0.2,change_labels=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train['message'], df_train['sentiment'], test_size=0.2, random_state=42, stratify=df_train['sentiment'])\n",
    "\n",
    "# Set common preprocessing parameters to include in GridSearch\n",
    "preprocessing_params = {\n",
    "    'tokenization__type': ['TweetTokenizer','TreebankWordTokenizer'],\n",
    "    'lemmatization__pos': ['v'],\n",
    "    'vectorization__type': ['count','tfidf'],\n",
    "    'vectorization__max_df': [0.25,0.5,0.75],\n",
    "    'vectorization__min_df': [1],\n",
    "    'vectorization__ngram_range': [(1,1),(1,3)],\n",
    "    'vectorization__max_features': [None],\n",
    "    'scaler__type': ['robust'],\n",
    "    'smote__k_neighbors': [3],\n",
    "    'smote__random_state': [42],\n",
    "    'feature_selection__percentile': [10, 99],\n",
    "    'feature_selection__type': ['anova_f']\n",
    "}\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('noise_removal', NoiseRemover()),\n",
    "    ('emoticon_convertion', EmoticonConverter()),\n",
    "    ('punctuation_removal', PunctuationRemover()),\n",
    "    ('tokenization', Tokenizer()),\n",
    "    ('garbageout', GarbageRemover()),\n",
    "    ('stopword_removal', StopwordRemover()),\n",
    "    ('lemmatization', Lemmatizer()),\n",
    "    ('vectorization', Vectorize()),\n",
    "    #('oversampler', RandomOverSampler(sampling_strategy=oversampling_strategy, random_state=42)),\n",
    "    #('undersampler', RandomUnderSampler(sampling_strategy=undersampling_strategy, random_state=42)),\n",
    "    ('smote', SMOTE()),\n",
    "    ('scaler', Scaler()),\n",
    "    ('feature_selection', FeatureSelector()),\n",
    "    ('model', None) #Placeholder for the model to be used\n",
    "])\n",
    "\n",
    "# Define the various models and their hyperpameters to include in Gridsearch\n",
    "model_params = {\n",
    "    'Multinomial Naive Bayes': {\n",
    "        **preprocessing_params,\n",
    "        'model': [MultinomialNB()],\n",
    "        'model__alpha': [0.01, 0.1, 1],\n",
    "        'model__fit_prior': [True, False]\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        **preprocessing_params,\n",
    "        'model': [LogisticRegression()],\n",
    "        'model__penalty': ['l2'],\n",
    "        'model__C': [0.001, 0.1, 1, 10],\n",
    "        'model__solver': ['sag'],\n",
    "        'model__multi_class': ['multinomial'],\n",
    "        'model__max_iter': [1000000],\n",
    "        'model__tol': [0.0001, 0.00001],\n",
    "        'model__random_state': [42]\n",
    "    }\n",
    "}\n",
    "\n",
    "for model_name, param_grid in model_params.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    # Update the pipeline with the classifier\n",
    "    pipeline.set_params(model=None)  # Reset the classifier\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1_macro', error_score='raise', verbose=2)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    df_gridsearch = pd.DataFrame(grid_search.cv_results_)\n",
    "    # Write to CSV file\n",
    "    selected_columns = ['Model','Dataset','Timestamp','mean_fit_time','mean_score_time','params','split0_test_score','split1_test_score','split2_test_score','split3_test_score','split4_test_score','std_test_score','mean_test_score']\n",
    "    #Create blank results dataframe - only do this once\n",
    "    #results_df = pd.DataFrame(columns=selected_columns)\n",
    "    #results_df.to_csv('/content/drive/MyDrive/Professionele ontwikkeling/Data Science/Explore Data Science Course/Sprint 6_Advanced Classification/Predict/advanced-classification-predict/notebook/Model_Comparison results.csv', index=False)\n",
    "    new_data = pd.DataFrame(columns=selected_columns)\n",
    "    new_data[['mean_fit_time','mean_score_time','params','split0_test_score','split1_test_score',\n",
    "         'split2_test_score','split3_test_score','split4_test_score','std_test_score','mean_test_score']] = df_gridsearch[['mean_fit_time',\n",
    "         'mean_score_time','params','split0_test_score','split1_test_score','split2_test_score','split3_test_score','split4_test_score','std_test_score','mean_test_score']]\n",
    "    new_data['Model'] = model_name\n",
    "    new_data['Dataset'] = size\n",
    "    new_data['Timestamp']= datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    new_data.to_csv('/content/drive/MyDrive/Professionele ontwikkeling/Data Science/Explore Data Science Course/Sprint 6_Advanced Classification/Predict/advanced-classification-predict/notebook/Model_Comparison results.csv', mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 67686,
     "status": "ok",
     "timestamp": 1713004022351,
     "user": {
      "displayName": "Dawie Loots",
      "userId": "00798578190767538914"
     },
     "user_tz": -120
    },
    "id": "NRLEyJBe95dA",
    "outputId": "e6faa2f9-6708-4f36-f59f-0f6b7c7e622a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Classification Report for test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Anti       0.53      0.59      0.56       278\n",
      "     Neutral       0.47      0.43      0.45       425\n",
      "        News       0.69      0.81      0.74       706\n",
      "         Pro       0.80      0.75      0.78      1755\n",
      "\n",
      "    accuracy                           0.71      3164\n",
      "   macro avg       0.62      0.64      0.63      3164\n",
      "weighted avg       0.71      0.71      0.71      3164\n",
      "\n",
      "************************\n",
      "Classification Report for CV:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Anti       0.44      0.50      0.47      1018\n",
      "     Neutral       0.49      0.40      0.44      1928\n",
      "        News       0.69      0.78      0.73      2934\n",
      "         Pro       0.78      0.76      0.77      6775\n",
      "\n",
      "    accuracy                           0.69     12655\n",
      "   macro avg       0.60      0.61      0.60     12655\n",
      "weighted avg       0.69      0.69      0.69     12655\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train model on 100% of the data\n",
    "\n",
    "df_train, _, _ = load_dataset('colab',1,change_labels=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train['message'], df_train['sentiment'], test_size=0.2, random_state=42)#, stratify=df_train['sentiment'])\n",
    "preprocessing_steps =  [\n",
    "                       ('noise_removal', NoiseRemover()),\n",
    "                       ('emoticon_convertion', EmoticonConverter()),\n",
    "                       ('punctuation_removal', PunctuationRemover()),\n",
    "                       ('tokenization', Tokenizer(type='TweetTokenizer')),\n",
    "                       ('garbageout', GarbageRemover()),\n",
    "                       ('stopword_removal', StopwordRemover()),\n",
    "                       ('lemmatization', Lemmatizer(pos='v')),\n",
    "                       ('vectorization', Vectorize(max_df=0.25,min_df=1, max_features=None,ngram_range=(1,1), type='count')),\n",
    "                       #('oversampler', RandomOverSampler(sampling_strategy=oversampling_strategy, random_state=42)),\n",
    "                       #('undersampler', RandomUnderSampler(sampling_strategy=undersampling_strategy, random_state=42)),\n",
    "                       ('smote', SMOTE(k_neighbors=3, random_state=42)),\n",
    "                       ('scaler', Scaler(type='robust')),\n",
    "                       ('feature_selection', FeatureSelector(percentile=99, type='anova_f', ))\n",
    "                       ]\n",
    "\n",
    "model = MultinomialNB(fit_prior=True, alpha=0.1)\n",
    "pipeline = Pipeline(preprocessing_steps  + [('model', model)])\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report for test set:\")\n",
    "print(report)\n",
    "print('************************')\n",
    "\n",
    "k_fold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "predictions = cross_val_predict(pipeline, X_train, y_train, cv=k_fold)\n",
    "report = classification_report(y_train, predictions)\n",
    "print(\"Classification Report for CV:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5fuiWQY24err"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Evaluate the pipeline\n",
    "accuracy = pipeline.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "model_stats = pipeline.named_steps['model']\n",
    "\n",
    "# Obtain the number of features used by the model\n",
    "attributes = model_stats.__dict__\n",
    "\n",
    "# Print all attributes\n",
    "for attr, value in attributes.items():\n",
    "    print(attr, \":\", value)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FaIL1W_gpHrG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQoIB6k0XiRF"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f'G:/MyDrive/Professionele ontwikkeling/Data Science/Explore Data Science Course/Sprint 6_Advanced Classification/Predict/advanced-classification-predict/notebook/dawieloots_predict_gridsearch_{timestamp}.csv'\n",
    "df_gridsearch.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_lxVdk5DJ4Ci"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(df_gridsearch)\n",
    "print(grid_search.get_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6fbMy5RktY2"
   },
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Custom transformer to wrap SMOTE\n",
    "\n",
    "class ResampleAndFeatureSelectTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, k=1000, score_func=chi2, k_neighbors=5):\n",
    "        self.k = k\n",
    "        self.score_func = score_func\n",
    "        self.k_neighbors = k_neighbors\n",
    "        self.feature_selector = SelectKBest(score_func=self.score_func, k=self.k)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Print the shape of X_train before preprocessing\n",
    "               \n",
    "        # Select features from the resampled data\n",
    "        self.feature_selector.fit(X, y)\n",
    "        \n",
    "        # Print the shape of X_train after preprocessing\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Select features from the input data\n",
    "        X_selected = self.feature_selector.transform(X)\n",
    "        return X_selected\n",
    "\n",
    "# Load the 20 Newsgroups dataset\n",
    "data = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "data_subset = {\n",
    "    'data': data.data[:1000],\n",
    "    'target': data.target[:1000],\n",
    "    'target_names': data.target_names\n",
    "}\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_subset['data'], data_subset['target'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Define preprocessing steps\n",
    "preprocessing_steps = [\n",
    "    # Text preprocessing\n",
    "    ('vectorizer', TfidfVectorizer()),  # Convert text data into numerical vectors\n",
    "    ('smote', SMOTE()),\n",
    "    ('resample_and_feature_select', ResampleAndFeatureSelectTransformer(k=1000, score_func=chi2)),  # Resample and select top k features\n",
    "    \n",
    "]\n",
    "\n",
    "# Define the model\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(preprocessing_steps + [('model', model)])\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the pipeline\n",
    "accuracy = pipeline.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "model = pipeline.named_steps['model']\n",
    "\n",
    "# Obtain the number of features used by the model\n",
    "attributes = model.__dict__\n",
    "\n",
    "# Print all attributes\n",
    "for attr, value in attributes.items():\n",
    "    print(attr, \":\", value)\n",
    "#n_features = model.n_features_\n",
    "#print(\"Number of features used by the model:\", n_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 506
    },
    "executionInfo": {
     "elapsed": 19038,
     "status": "ok",
     "timestamp": 1713003495324,
     "user": {
      "displayName": "Dawie Loots",
      "userId": "00798578190767538914"
     },
     "user_tz": -120
    },
    "id": "AyoIXSk4rU58",
    "outputId": "34ae850b-2dc7-4fd3-8904-6c48bdeb311a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;noise_removal&#x27;, NoiseRemover()),\n",
       "                (&#x27;emoticon_convertion&#x27;, EmoticonConverter()),\n",
       "                (&#x27;punctuation_removal&#x27;, PunctuationRemover()),\n",
       "                (&#x27;tokenization&#x27;, Tokenizer(type=&#x27;TweetTokenizer&#x27;)),\n",
       "                (&#x27;garbageout&#x27;, GarbageRemover()),\n",
       "                (&#x27;stopword_removal&#x27;, StopwordRemover()),\n",
       "                (&#x27;lemmatization&#x27;, Lemmatizer(pos=&#x27;v&#x27;)),\n",
       "                (&#x27;vectorization&#x27;,\n",
       "                 Vectorize(max_df=0.25, min_df=1, ngram_range=(1, 1),\n",
       "                           type=&#x27;count&#x27;)),\n",
       "                (&#x27;smote&#x27;, SMOTE(k_neighbors=3, random_state=42)),\n",
       "                (&#x27;scaler&#x27;, Scaler(type=&#x27;robust&#x27;)),\n",
       "                (&#x27;feature_selection&#x27;,\n",
       "                 FeatureSelector(percentile=99, type=&#x27;anova_f&#x27;)),\n",
       "                (&#x27;model&#x27;, MultinomialNB(alpha=0.1))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;noise_removal&#x27;, NoiseRemover()),\n",
       "                (&#x27;emoticon_convertion&#x27;, EmoticonConverter()),\n",
       "                (&#x27;punctuation_removal&#x27;, PunctuationRemover()),\n",
       "                (&#x27;tokenization&#x27;, Tokenizer(type=&#x27;TweetTokenizer&#x27;)),\n",
       "                (&#x27;garbageout&#x27;, GarbageRemover()),\n",
       "                (&#x27;stopword_removal&#x27;, StopwordRemover()),\n",
       "                (&#x27;lemmatization&#x27;, Lemmatizer(pos=&#x27;v&#x27;)),\n",
       "                (&#x27;vectorization&#x27;,\n",
       "                 Vectorize(max_df=0.25, min_df=1, ngram_range=(1, 1),\n",
       "                           type=&#x27;count&#x27;)),\n",
       "                (&#x27;smote&#x27;, SMOTE(k_neighbors=3, random_state=42)),\n",
       "                (&#x27;scaler&#x27;, Scaler(type=&#x27;robust&#x27;)),\n",
       "                (&#x27;feature_selection&#x27;,\n",
       "                 FeatureSelector(percentile=99, type=&#x27;anova_f&#x27;)),\n",
       "                (&#x27;model&#x27;, MultinomialNB(alpha=0.1))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NoiseRemover</label><div class=\"sk-toggleable__content\"><pre>NoiseRemover()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">EmoticonConverter</label><div class=\"sk-toggleable__content\"><pre>EmoticonConverter()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PunctuationRemover</label><div class=\"sk-toggleable__content\"><pre>PunctuationRemover()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Tokenizer</label><div class=\"sk-toggleable__content\"><pre>Tokenizer(type=&#x27;TweetTokenizer&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GarbageRemover</label><div class=\"sk-toggleable__content\"><pre>GarbageRemover()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StopwordRemover</label><div class=\"sk-toggleable__content\"><pre>StopwordRemover()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Lemmatizer</label><div class=\"sk-toggleable__content\"><pre>Lemmatizer(pos=&#x27;v&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Vectorize</label><div class=\"sk-toggleable__content\"><pre>Vectorize(max_df=0.25, min_df=1, ngram_range=(1, 1), type=&#x27;count&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SMOTE</label><div class=\"sk-toggleable__content\"><pre>SMOTE(k_neighbors=3, random_state=42)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Scaler</label><div class=\"sk-toggleable__content\"><pre>Scaler(type=&#x27;robust&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FeatureSelector</label><div class=\"sk-toggleable__content\"><pre>FeatureSelector(percentile=99, type=&#x27;anova_f&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB(alpha=0.1)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('noise_removal', NoiseRemover()),\n",
       "                ('emoticon_convertion', EmoticonConverter()),\n",
       "                ('punctuation_removal', PunctuationRemover()),\n",
       "                ('tokenization', Tokenizer(type='TweetTokenizer')),\n",
       "                ('garbageout', GarbageRemover()),\n",
       "                ('stopword_removal', StopwordRemover()),\n",
       "                ('lemmatization', Lemmatizer(pos='v')),\n",
       "                ('vectorization',\n",
       "                 Vectorize(max_df=0.25, min_df=1, ngram_range=(1, 1),\n",
       "                           type='count')),\n",
       "                ('smote', SMOTE(k_neighbors=3, random_state=42)),\n",
       "                ('scaler', Scaler(type='robust')),\n",
       "                ('feature_selection',\n",
       "                 FeatureSelector(percentile=99, type='anova_f')),\n",
       "                ('model', MultinomialNB(alpha=0.1))])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Final model building with full trainset\n",
    "df_train, _, _ = load_dataset('colab',1,change_labels=True)\n",
    "X_train = df_train['message']\n",
    "y_train = df_train['sentiment']\n",
    "\n",
    "preprocessing_steps =  [\n",
    "                       ('noise_removal', NoiseRemover()),\n",
    "                       ('emoticon_convertion', EmoticonConverter()),\n",
    "                       ('punctuation_removal', PunctuationRemover()),\n",
    "                       ('tokenization', Tokenizer(type='TweetTokenizer')),\n",
    "                       ('garbageout', GarbageRemover()),\n",
    "                       ('stopword_removal', StopwordRemover()),\n",
    "                       ('lemmatization', Lemmatizer(pos='v')),\n",
    "                       ('vectorization', Vectorize(max_df=0.25,min_df=1, max_features=None,ngram_range=(1,1), type='count')),\n",
    "                       #('oversampler', RandomOverSampler(sampling_strategy=oversampling_strategy, random_state=42)),\n",
    "                       #('undersampler', RandomUnderSampler(sampling_strategy=undersampling_strategy, random_state=42)),\n",
    "                       ('smote', SMOTE(k_neighbors=3, random_state=42)),\n",
    "                       ('scaler', Scaler(type='robust')),\n",
    "                       ('feature_selection', FeatureSelector(percentile=99, type='anova_f', ))\n",
    "                       ]\n",
    "\n",
    "#model = SVC(C=10,gamma=0.01)  #THIS SEEMS TO BE THE BEST\n",
    "#model = LogisticRegression(C=1, max_iter=1000000,multi_class='multinomial', solver='sag', tol=0.001, penalty='l2', random_state=42)\n",
    "model = MultinomialNB(fit_prior=True, alpha=0.1)\n",
    "pipeline = Pipeline(preprocessing_steps  + [('model', model)])\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1712856834359,
     "user": {
      "displayName": "Dawie Loots",
      "userId": "00798578190767538914"
     },
     "user_tz": -120
    },
    "id": "rkdRn-om_QiA",
    "outputId": "0f009148-eae8-4f39-bae7-553b243ce5c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "penalty : l2\n",
      "dual : False\n",
      "tol : 0.001\n",
      "C : 1\n",
      "fit_intercept : True\n",
      "intercept_scaling : 1\n",
      "class_weight : None\n",
      "random_state : 42\n",
      "solver : sag\n",
      "max_iter : 1000000\n",
      "multi_class : multinomial\n",
      "verbose : 0\n",
      "warm_start : False\n",
      "n_jobs : None\n",
      "l1_ratio : None\n",
      "n_features_in_ : 22481\n",
      "classes_ : [-1  0  1  2]\n",
      "n_iter_ : [55]\n",
      "coef_ : [[-0.09849407 -0.06170661 -0.0242912  ... -0.0316989  -0.0316989\n",
      "  -0.04595831]\n",
      " [ 0.47132916 -0.01673616 -0.0268516  ...  0.17909438  0.17909438\n",
      "   0.18485926]\n",
      " [-0.11636423  0.17776388  0.06536473 ... -0.0932192  -0.0932192\n",
      "  -0.0767753 ]\n",
      " [-0.25647085 -0.09932111 -0.01422193 ... -0.05417628 -0.05417628\n",
      "  -0.06212565]]\n",
      "intercept_ : [-0.73788254  1.18096025 -0.08638792 -0.35668979]\n"
     ]
    }
   ],
   "source": [
    "model_stats = pipeline.named_steps['model']\n",
    "\n",
    "# Obtain the number of features used by the model\n",
    "attributes = model_stats.__dict__\n",
    "\n",
    "# Print all attributes\n",
    "for attr, value in attributes.items():\n",
    "    print(attr, \":\", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T6DW763d3M--"
   },
   "outputs": [],
   "source": [
    "test_file = '/content/drive/MyDrive/Professionele ontwikkeling/Data Science/Explore Data Science Course/Sprint 6_Advanced Classification/Predict/advanced-classification-predict/data/test_with_no_labels.csv'\n",
    "df_test = pd.read_csv(test_file)\n",
    "X_unseen = df_test['message']\n",
    "\n",
    "# Predict\n",
    "y_pred_text = pipeline.predict(X_unseen)\n",
    "label_map = {\n",
    "        'News': 2,\n",
    "        'Pro': 1,\n",
    "        'Neutral': 0,\n",
    "        'Anti': -1\n",
    "    }\n",
    "y_pred_int = [label_map[label] for label in y_pred_text]\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f'/content/drive/MyDrive/Professionele ontwikkeling/Data Science/Explore Data Science Course/Sprint 6_Advanced Classification/Predict/advanced-classification-predict/notebook/dawieloots_predict_{timestamp}.csv'\n",
    "submission_df = pd.DataFrame()\n",
    "submission_df['tweetid'] = df_test.tweetid\n",
    "submission_df['sentiment'] = y_pred_int\n",
    "submission_df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1713004123896,
     "user": {
      "displayName": "Dawie Loots",
      "userId": "00798578190767538914"
     },
     "user_tz": -120
    },
    "id": "eUfcXvWWADJm",
    "outputId": "3ed4ce8c-ec40-424c-b14b-1890ab20e66f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"submission_df\",\n  \"rows\": 10546,\n  \"fields\": [\n    {\n      \"column\": \"tweetid\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 288115,\n        \"min\": 231,\n        \"max\": 999983,\n        \"num_unique_values\": 10546,\n        \"samples\": [\n          495618,\n          845259,\n          771517\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": -1,\n        \"max\": 2,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0,\n          -1,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "submission_df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-a241e4ab-a5d2-4e30-9c54-b8d23cfe38eb\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetid</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>169760</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35326</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>224985</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>476263</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>872928</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a241e4ab-a5d2-4e30-9c54-b8d23cfe38eb')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-a241e4ab-a5d2-4e30-9c54-b8d23cfe38eb button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-a241e4ab-a5d2-4e30-9c54-b8d23cfe38eb');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-d6035c3a-e423-4a70-b182-5c51114c6cc2\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d6035c3a-e423-4a70-b182-5c51114c6cc2')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-d6035c3a-e423-4a70-b182-5c51114c6cc2 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "   tweetid  sentiment\n",
       "0   169760          1\n",
       "1    35326          1\n",
       "2   224985          1\n",
       "3   476263          1\n",
       "4   872928          0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df.head()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1OxjFEie5eThMMpfvLVib244h1m4KVxzQ",
     "timestamp": 1713013043819
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

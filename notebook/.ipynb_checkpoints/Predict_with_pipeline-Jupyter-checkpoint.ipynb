{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4417,"status":"ok","timestamp":1712574199835,"user":{"displayName":"Dawie Loots","userId":"00798578190767538914"},"user_tz":-120},"id":"h20rlqi05cIw","outputId":"73c6571b-e468-4f20-f144-a77ebf1b23d3"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]}],"source":["# Libraries for data loading, data manipulation and data visulisation\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import chardet # To provide a best estimate of the encoding that was used in the text data\n","import io # For string operations\n","%matplotlib inline\n","\n","# Libraries for data preparation and model building\n","#from imblearn.over_sampling import RandomOverSampler\n","import nltk\n","nltk.download('stopwords')\n","from nltk.tokenize import word_tokenize, TweetTokenizer, TreebankWordTokenizer\n","from nltk.corpus import stopwords\n","import string\n","from nltk.stem import WordNetLemmatizer\n","nltk.download('wordnet')\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from gensim.models import Word2Vec\n","import math\n","import re\n","from sklearn.utils import resample\n","from scipy.sparse import vstack\n","from scipy.sparse import csr_matrix\n","from sklearn import metrics\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn import feature_selection\n","from sklearn.feature_selection import SelectPercentile, SelectKBest, f_classif, mutual_info_classif\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n","import xgboost as xgb\n","from sklearn.naive_bayes import GaussianNB, MultinomialNB\n","from sklearn.metrics import classification_report, accuracy_score, log_loss\n","from sklearn.model_selection import GridSearchCV, KFold, StratifiedKFold\n","\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.feature_selection import SelectFromModel\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import FunctionTransformer\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.metrics import classification_report\n","from imblearn.over_sampling import SMOTE,SMOTENC\n","from imblearn.pipeline import Pipeline"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","df_train = pd.read_csv('/content/drive/MyDrive/Professionele ontwikkeling/Data Science/Explore Data Science Course/Sprint 6_Advanced Classification/Predict/advanced-classification-predict/data/train.csv')\n","pd.set_option('display.max_colwidth', None)\n","df_train.head(10)"],"metadata":{"id":"2hMYSMSB6fHp","colab":{"base_uri":"https://localhost:8080/","height":380},"executionInfo":{"status":"ok","timestamp":1712574229752,"user_tz":-120,"elapsed":29925,"user":{"displayName":"Dawie Loots","userId":"00798578190767538914"}},"outputId":"7b9aa4f1-abee-4b26-fc12-ef935c7c2ba3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]},{"output_type":"execute_result","data":{"text/plain":["   sentiment  \\\n","0          1   \n","1          1   \n","2          2   \n","3          1   \n","4          1   \n","5          1   \n","6          1   \n","7          1   \n","8          1   \n","9          1   \n","\n","                                                                                                                                                    message  \\\n","0              PolySciMajor EPA chief doesn't think carbon dioxide is main cause of global warming and.. wait, what!? https://t.co/yeLvcEFXkC via @mashable   \n","1                                                                                            It's not like we lack evidence of anthropogenic global warming   \n","2              RT @RawStory: Researchers say we have three years to act on climate change before it’s too late https://t.co/WdT0KdUr2f https://t.co/Z0ANPT…   \n","3                                                       #TodayinMaker# WIRED : 2016 was a pivotal year in the war on climate change https://t.co/44wOTxTLcD   \n","4                                RT @SoyNovioDeTodas: It's 2016, and a racist, sexist, climate change denying bigot is leading in the polls. #ElectionNight   \n","5                                            Worth a read whether you do or don't believe in climate change https://t.co/ggLZVNYjun https://t.co/7AFE2mAH8j   \n","6                                   RT @thenation: Mike Pence doesn’t believe in global warming or that smoking causes lung cancer. https://t.co/gvWYaauU8R   \n","7  RT @makeandmendlife: Six big things we can ALL do today to fight climate change, or how to be a climate activistÃ¢â‚¬Â¦ https://t.co/TYMLu6DbNM hÃ¢â‚¬Â¦   \n","8               @AceofSpadesHQ My 8yo nephew is inconsolable. He wants to die of old age like me, but will perish in the fiery hellscape of climate change.   \n","9                                                                  RT @paigetweedy: no offense… but like… how do you just not believe… in global warming………   \n","\n","   tweetid  \n","0   625221  \n","1   126103  \n","2   698562  \n","3   573736  \n","4   466954  \n","5   425577  \n","6   294933  \n","7   992717  \n","8   664510  \n","9   260471  "],"text/html":["\n","  <div id=\"df-a66a5cb4-938e-4437-a399-9a8b423d6c3e\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentiment</th>\n","      <th>message</th>\n","      <th>tweetid</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>PolySciMajor EPA chief doesn't think carbon dioxide is main cause of global warming and.. wait, what!? https://t.co/yeLvcEFXkC via @mashable</td>\n","      <td>625221</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>It's not like we lack evidence of anthropogenic global warming</td>\n","      <td>126103</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>RT @RawStory: Researchers say we have three years to act on climate change before it’s too late https://t.co/WdT0KdUr2f https://t.co/Z0ANPT…</td>\n","      <td>698562</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>#TodayinMaker# WIRED : 2016 was a pivotal year in the war on climate change https://t.co/44wOTxTLcD</td>\n","      <td>573736</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, sexist, climate change denying bigot is leading in the polls. #ElectionNight</td>\n","      <td>466954</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1</td>\n","      <td>Worth a read whether you do or don't believe in climate change https://t.co/ggLZVNYjun https://t.co/7AFE2mAH8j</td>\n","      <td>425577</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>1</td>\n","      <td>RT @thenation: Mike Pence doesn’t believe in global warming or that smoking causes lung cancer. https://t.co/gvWYaauU8R</td>\n","      <td>294933</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>1</td>\n","      <td>RT @makeandmendlife: Six big things we can ALL do today to fight climate change, or how to be a climate activistÃ¢â‚¬Â¦ https://t.co/TYMLu6DbNM hÃ¢â‚¬Â¦</td>\n","      <td>992717</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>1</td>\n","      <td>@AceofSpadesHQ My 8yo nephew is inconsolable. He wants to die of old age like me, but will perish in the fiery hellscape of climate change.</td>\n","      <td>664510</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>1</td>\n","      <td>RT @paigetweedy: no offense… but like… how do you just not believe… in global warming………</td>\n","      <td>260471</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a66a5cb4-938e-4437-a399-9a8b423d6c3e')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-a66a5cb4-938e-4437-a399-9a8b423d6c3e button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-a66a5cb4-938e-4437-a399-9a8b423d6c3e');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-9488e48b-04e9-4cf1-a5c3-4ea90ef2aee6\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9488e48b-04e9-4cf1-a5c3-4ea90ef2aee6')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-9488e48b-04e9-4cf1-a5c3-4ea90ef2aee6 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df_train","summary":"{\n  \"name\": \"df_train\",\n  \"rows\": 15819,\n  \"fields\": [\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": -1,\n        \"max\": 2,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2,\n          -1,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"message\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 14229,\n        \"samples\": [\n          \"UK: Keep your climate change and wildlife commitments https://t.co/4SMhOQkoeZ\",\n          \"The 'simple question' that can change your mind about global warming - CNN https://t.co/DWJhULn33N\",\n          \"#IntriguingRead: Shell made a film about climate change in 1991 (then neglected to heed its own warning) https://t.co/2arIOouGCA\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tweetid\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 289045,\n        \"min\": 6,\n        \"max\": 999888,\n        \"num_unique_values\": 15819,\n        \"samples\": [\n          697253,\n          94360,\n          704751\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","source":["df_train = pd.read_csv('G:/MyDrive/Professionele ontwikkeling/Data Science/Explore Data Science Course/Sprint 6_Advanced Classification/Predict/advanced-classification-predict/data/train.csv')\n","pd.set_option('display.max_colwidth', None)\n","df_train.head(10)"],"metadata":{"id":"LUVu-k0BXiQ3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"eNAlbB5V6nag"},"outputs":[],"source":["# Step 1: Split into train / test\n","X_train, X_test, y_train, y_test = train_test_split(df_train['message'], df_train['sentiment'], test_size=0.2, random_state=42)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":54,"status":"ok","timestamp":1712574229753,"user":{"displayName":"Dawie Loots","userId":"00798578190767538914"},"user_tz":-120},"id":"IxK0uFc4S76T","outputId":"616bbcfc-1aa4-4dae-e43b-7c3dbfa74aaf"},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.series.Series'>\n","Index: 12655 entries, 6735 to 7270\n","Series name: message\n","Non-Null Count  Dtype \n","--------------  ----- \n","12655 non-null  object\n","dtypes: object(1)\n","memory usage: 197.7+ KB\n"]}],"source":["X_train.info()"]},{"cell_type":"markdown","metadata":{"id":"DCJ0bOA1TCmK"},"source":["https://www.andrewvillazon.com/custom-scikit-learn-transformers/\n","\n","SIEN OOK PDF IN DOWNLOADS: How To Write Clean And Scalable Code With Custom Transformers & Sklearn Pipelines _ by Beng Chew _ Medium\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2gOiuldv5PDz"},"outputs":[],"source":["class SampleSelector(BaseEstimator, TransformerMixin):\n","    def __init__(self, sample_percentage=1.0, stratify='y', random_state=42):\n","        self.sample_percentage = sample_percentage\n","        self.stratify = stratify\n","        self.random_state = random_state\n","\n","    def fit(self, X, y):\n","        return self\n","\n","    def transform(self, X, y=None):\n","        # Set random seed for reproducibility\n","        np.random.seed(self.random_state)\n","        if self.sample_percentage != 1.0:\n","            print(X.shape)\n","            print(y.shape)\n","            print(f'y:')\n","            print(y.iloc[0])\n","            # Determine the unique classes in y and their frequencies\n","            unique_classes, class_counts = np.unique(y, return_counts=True)\n","            # Determine the number of samples to select for each class\n","            num_samples_per_class = (class_counts * self.sample_percentage).astype(int)\n","            # Initialize an empty list to store the sampled indices\n","            print(unique_classes)\n","            print(class_counts)\n","            sampled_indices = []\n","\n","            # Iterate over each unique class\n","            for cls, num_samples in zip(unique_classes, num_samples_per_class):\n","                # Get the indices of samples belonging to the current class\n","                class_indices = np.where(y == cls)[0]\n","                # Randomly select samples from the current class\n","                sampled_indices.extend(np.random.choice(class_indices, size=num_samples, replace=False))\n","\n","            print(sampled_indices)\n","            X_sampled = X.iloc[sampled_indices]\n","            y_sampled = y.iloc[sampled_indices]\n","\n","        else:\n","            X_sampled = X\n","            y_sampled = y\n","\n","        # Return the randomly sampled subset\n","\n","        print(type(X_sampled))\n","        print(f'X_sampled[0]: {X_sampled.iloc[0]}')\n","        print(f'y_sampled[0]: {y_sampled.iloc[0]}')\n","        return X_sampled, y_sampled\n","\n","    def fit_transform(self, X, y=None):\n","        self.fit(X, y)\n","        X_sampled = self.transform(X, y)\n","        return X_sampled, y_sampled"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1rI10jOolRbZ"},"outputs":[],"source":["class NoiseRemover(BaseEstimator, TransformerMixin):\n","    def __init__(self):\n","        pass\n","\n","    def fit(self, X, y):\n","        return self\n","\n","    def transform(self, X, y=None):\n","        pattern_url = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'   # Find all hyperlinks\n","        subs_url = r''\n","        X_transformed = pd.Series(X).replace(to_replace = pattern_url, value = subs_url, regex = True)\n","        return X_transformed\n","\n","    def fit_transform(self, X, y=None):\n","        self.fit(X, y)\n","        return self.transform(X, y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"npvvupgdqXbC"},"outputs":[],"source":["class EmoticonConverter(BaseEstimator, TransformerMixin):\n","    def __init__(self):\n","        pass\n","\n","    def fit(self, X, y):\n","        return self\n","\n","    def transform(self, X, y=None):\n","        emoticon_dictionary = {':\\)': 'smiley_face_emoticon',\n","                               ':\\(': 'frowning_face_emoticon',\n","                               ':D': 'grinning_face_emoticon',\n","                               ':P': 'sticking_out_tongue_emoticon',\n","                               ';\\)': 'winking_face_emoticon',\n","                               ':o': 'surprised_face_emoticon',\n","                               ':\\|': 'neutral_face_emoticon',\n","                               ':\\'\\)': 'tears_of_joy_emoticon',\n","                               ':\\'\\(': 'crying_face_emoticon'}\n","        X_transformed = X.replace(emoticon_dictionary, regex=True)\n","        return X_transformed\n","\n","    def fit_transform(self, X, y=None):\n","        self.fit(X, y)\n","        return self.transform(X, y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VTRod0QUsOL7"},"outputs":[],"source":["class PunctuationRemover(BaseEstimator, TransformerMixin):\n","    def __init__(self):\n","        pass\n","\n","    def fit(self, X, y):\n","        return self\n","\n","    def transform(self, X, y=None):\n","        #print(X.iloc[1])\n","        def expand_contractions(text):\n","            #print(f'X type during function expand_contractions: {type(text)}')\n","            contractions = {\"'t\": \" not\",\"'s\": \" is\",\"'re\": \" are\",\"'ll\": \" will\", \"'m\": \" am\"}\n","            pattern = re.compile(r\"\\b(\" + \"|\".join(re.escape(key) for key in contractions.keys()) + r\")\\b\")\n","            text = re.sub(r\"n't\\b\", \" not\", text) # Replace \"n't\" with \" not\"\n","            text = pattern.sub(lambda match: contractions[match.group(0)], text) # Replace all other contractions except for \"n't\"\n","            return text\n","\n","        def remove_punctuation(text):\n","            return ''.join([l for l in text if l not in string.punctuation])\n","\n","        #print(f'X type before transformation: {type(X)}')\n","        #X_transformed = X.apply(expand_contractions)\n","        X_transformed = X.apply(lambda x: expand_contractions(x))\n","        #print(f'X_transformed type after expand function: {type(X_transformed)}')\n","        #X_transformed = X_transformed.apply(remove_punctuation)\n","        X_transformed = X_transformed.apply(lambda x: remove_punctuation(x))\n","\n","\n","        return X_transformed\n","\n","    def fit_transform(self, X, y=None):\n","        self.fit(X, y)\n","        return self.transform(X, y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b7cme7kUuyIt"},"outputs":[],"source":["class Tokenizer(BaseEstimator, TransformerMixin):\n","    def __init__(self, type='TweetTokenizer'):\n","        self.type = type\n","\n","    def fit(self, X, y=None):\n","        return self\n","\n","    def transform(self, X, y=None):\n","        if self.type == 'TweetTokenizer':\n","            tokenizer = TweetTokenizer()\n","        else:\n","            tokenizer = TreebankWordTokenizer()\n","        X_transformed = X.apply(tokenizer.tokenize)\n","        return X_transformed\n","\n","    def fit_transform(self, X, y=None):\n","        self.fit(X, y)\n","        return self.transform(X, y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-puz8STKxpt3"},"outputs":[],"source":["class StopwordRemover(BaseEstimator, TransformerMixin):\n","    def __init__(self):\n","        pass\n","\n","    def fit(self, X, y=None):\n","        return self\n","\n","    def transform(self, X, y=None):\n","        stop_words = set(stopwords.words('english'))\n","        # Remove stopwords using a vectorized operation\n","        X_transformed = X.apply(lambda tokens: [t for t in tokens if t.lower() not in stop_words])\n","        return X_transformed\n","\n","    def fit_transform(self, X, y=None):\n","        self.fit(X, y)\n","        return self.transform(X, y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1miNSgYK4zfz"},"outputs":[],"source":["class Lemmatizer(BaseEstimator, TransformerMixin):\n","    def __init__(self, pos='v'):\n","        self.pos = pos\n","\n","    def fit(self, X, y=None):\n","        return self\n","\n","    def transform(self, X, y=None):\n","        lemmatizer = WordNetLemmatizer()\n","        X_transformed = X.apply(lambda tokens: [lemmatizer.lemmatize(word, pos=self.pos) for word in tokens])\n","        return X_transformed\n","\n","    def fit_transform(self, X, y=None):\n","        self.fit(X, y)\n","        return self.transform(X, y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XYw7dmak74hm"},"outputs":[],"source":["class Vectorize(BaseEstimator, TransformerMixin):\n","    def __init__(self, type='tfidf', max_df=1, min_df=1, ngram_range=(1,2), max_features=None):\n","        self.type = type\n","        self.max_df = max_df\n","        self.min_df = min_df\n","        self.ngram_range = ngram_range\n","        self.max_features = max_features\n","\n","        if self.type == 'count':\n","            self.vectorizer = CountVectorizer(max_features=self.max_features, lowercase=True,\n","                                              max_df=self.max_df, min_df=self.min_df,\n","                                              ngram_range=self.ngram_range)\n","        elif self.type == 'tfidf':\n","            self.vectorizer = TfidfVectorizer(max_features=self.max_features, lowercase=True,\n","                                              max_df=self.max_df, min_df=self.min_df,\n","                                              ngram_range=self.ngram_range)\n","        else:\n","            raise ValueError(\"Invalid vectorizer type. Choose either 'count' or 'tfidf'\")\n","\n","    def fit(self, X, y=None):\n","        X_joined = [' '.join(tokens) for tokens in X]\n","        self.vectorizer.fit(X_joined)\n","        return self\n","\n","    def transform(self, X, y=None):\n","        X_joined = [' '.join(tokens) for tokens in X]\n","        return self.vectorizer.transform(X_joined)\n","\n","    def fit_transform(self, X, y=None):\n","        self.fit(X, y)\n","        return self.transform(X, y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GtSr3DidJnGU"},"outputs":[],"source":["class Resampler(BaseEstimator, TransformerMixin):\n","    def __init__(self, technique='balanced', random_state=42):\n","        self.technique = technique\n","        self.random_state = random_state\n","\n","    def fit(self, X, y):\n","        return self\n","\n","    def transform(self, X, y=None):\n","        if self.technique == 'balanced':\n","            class_labels, counts = np.unique(y, return_counts=True)\n","            n_classes = class_labels.shape[0]\n","            balanced_freq = X.shape[0] / n_classes  # Get the number of samples from the shape of X\n","            X_resampled, y_resampled = [], []\n","            for class_label, count in zip(class_labels, counts):\n","                indices = np.atleast_1d(np.where(y == class_label)[0])\n","                if count > balanced_freq:\n","                    resampled_indices = resample(indices, replace=False, n_samples=int(balanced_freq), random_state=self.random_state)\n","                elif count < balanced_freq:\n","                    resampled_indices = resample(indices, replace=True, n_samples=int(balanced_freq), random_state=self.random_state)\n","                else:\n","                    resampled_indices = indices\n","                if len(resampled_indices) > 0:\n","                    resampled_X = X[resampled_indices]\n","                    X_resampled.append(resampled_X)\n","                    y_resampled.extend([class_label] * resampled_X.shape[0])\n","\n","            if X_resampled:\n","                X_resampled = vstack(X_resampled)\n","            else:\n","                X_resampled = csr_matrix((0, X.shape[1]))  # Create an empty sparse matrix\n","            y_resampled = np.array(y_resampled)\n","\n","        elif self.technique == 'smote':\n","            sampler = SMOTE(random_state=self.random_state)\n","            X_dense = X.toarray()\n","            X_resampled, y_resampled = sampler.fit_resample(X_dense, y)\n","            X_resampled = csr_matrix(X_resampled)\n","\n","        else:\n","            raise ValueError(\"Invalid resampling technique.  Choose either 'random' or 'smote'\")\n","\n","        return X_resampled, y_resampled\n","\n","    def fit_transform(self, X, y=None):\n","        self.fit(X, y)\n","        return self.transform(X, y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TP0o7OpfGpg3"},"outputs":[],"source":["class Scaler(BaseEstimator, TransformerMixin):\n","    def __init__(self, type='robust'):\n","        self.type = type\n","\n","        if self.type == 'robust':\n","            self.scaler = RobustScaler(with_centering=False)\n","        elif self.type == 'minmax':\n","            self.scaler = MinMaxScaler(with_centering=False)\n","        elif self.type == 'maxabs':\n","            self.scaler = MaxAbsScaler(with_centering=False)\n","        else:\n","            raise ValueError(\"Invalid scaler type. Choose between 'robust', 'minmax' or 'maxabs'.\")\n","\n","    def fit(self, X, y=None):\n","        self.scaler.fit(X)\n","        return self\n","\n","    def transform(self, X, y=None):\n","        return self.scaler.transform(X)\n","\n","    def fit_transform(self, X, y=None):\n","        self.fit(X, y)\n","        return self.transform(X, y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R5UyERuHxKKI"},"outputs":[],"source":["class FeatureSelector(BaseEstimator, TransformerMixin):\n","    def __init__(self, type='anova_f', percentile=50):\n","        self.type = type\n","        self.percentile = percentile\n","\n","        if self.type == 'mutualinfo':\n","            self.selector = SelectPercentile(score_func=mutual_info_classif, percentile=percentile)\n","        elif self.type == 'anova_f':\n","            self.selector = SelectPercentile(score_func=f_classif, percentile=percentile)\n","        else:\n","            raise ValueError(\"Invalid selector type. Choose between 'mutualinfo' or 'anova_f'.\")\n","\n","    def fit(self, X, y=None):\n","        print(y.head())\n","        self.selector.fit(X, y)\n","        return self\n","\n","    def transform(self, X, y=None):\n","        return self.selector.transform(X)\n","\n","    def fit_transform(self, X, y=None):\n","        self.selector.fit(X, y)\n","        return self.selector.transform(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5fVqF62W5XKf","outputId":"fc2ca114-a359-4af0-b40c-9e9c50124043"},"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 5 folds for each of 32 candidates, totalling 160 fits\n","[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, model__n_neighbors=3, scaler__type=robust, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=100, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=  36.6s\n","[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, model__n_neighbors=3, scaler__type=robust, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=100, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=  27.5s\n","[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, model__n_neighbors=3, scaler__type=robust, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=100, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=  27.2s\n","[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, model__n_neighbors=3, scaler__type=robust, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=100, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=  28.4s\n","[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, model__n_neighbors=3, scaler__type=robust, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=100, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=tfidf; total time=  27.0s\n","[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, model__n_neighbors=3, scaler__type=robust, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=100, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=CountVectorizer; total time=  27.3s\n","[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, model__n_neighbors=3, scaler__type=robust, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=100, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=CountVectorizer; total time=  27.8s\n","[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, model__n_neighbors=3, scaler__type=robust, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=100, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=CountVectorizer; total time=  27.4s\n","[CV] END feature_selection__percentile=25, feature_selection__type=anova_f, model__n_neighbors=3, scaler__type=robust, tokenization__type=TweetTokenizer, vectorization__max_df=0.25, vectorization__max_features=100, vectorization__min_df=1, vectorization__ngram_range=(1, 1), vectorization__type=CountVectorizer; total time=  26.6s\n"]}],"source":["X_train, X_test, y_train, y_test = train_test_split(df_train['message'], df_train['sentiment'], test_size=0.15, random_state=42)\n","# Defining preprocessing steps\n","preprocessing_steps = [\n","                       ('noise_removal', NoiseRemover()),\n","                       ('emoticon_convertion', EmoticonConverter()),\n","                       ('punctuation_removal', PunctuationRemover()),\n","                       ('tokenization', Tokenizer()),\n","                       ('stopword_removal', StopwordRemover()),\n","                       ('lemmatization', Lemmatizer()),\n","                       ('vectorization', Vectorize()),\n","                       ('smote', SMOTE()),\n","                       ('scaler', Scaler()),\n","                       ('feature_selection', FeatureSelector())\n","                       ]\n","\n","# Defining the models\n","models =               {\n","                        'Logistic Regression': LogisticRegression(),\n","                        'Support Vector Machine': SVC(),\n","                        'Multinomial Naive Bayes': MultinomialNB(),\n","                        'Decision Tree': DecisionTreeClassifier(),\n","                        'K-Nearest Neighbors': KNeighborsClassifier(),\n","                        'Random Forest': RandomForestClassifier(),\n","                       }\n","\n","# Create the pipelines\n","pipeline =              Pipeline(preprocessing_steps  + [('model', KNeighborsClassifier())])\n","\n","#pipeline.fit(X_train, y_train)\n","\n","# Define parameter grid for GridSearchCV\n","param_grid = {'tokenization__type': ['TweetTokenizer', 'TreebankWordTokenizer'],\n","              'vectorization__type': ['tfidf','CountVectorizer'],\n","              'vectorization__max_df': [0.25],#,0.5,0.75],\n","              'vectorization__min_df': [1,2],#10],\n","              'vectorization__ngram_range': [(1,1), (1,2)],#, (1,3)],\n","              'vectorization__max_features': [100, 200],# 500],#, 1000],\n","              'scaler__type': ['robust'],#'minmax','maxabs'],\n","              'feature_selection__type': ['anova_f'],\n","              'feature_selection__percentile': [25],#,50,75],\n","\n","            # Hyperparameters for the model\n","\n","              'model__n_neighbors': [3],\n","              #'model__weights': ['uniform', 'distance'],\n","              #'model__metric': ['euclidean', 'manhattan'],\n","\n","}\n","\n","# Create GridSearchCV object\n","grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1_macro', error_score='raise', verbose=2)\n","grid_search.fit(X_train, y_train)\n","df_gridsearch = pd.DataFrame(grid_search.cv_results_)\n","\n","print(df_gridsearch)\n","print(grid_search.get_params)\n","# print to Excel\n","'''\n","# Evaluate the pipeline\n","accuracy = pipeline.score(X_test, y_test)\n","print(\"Accuracy:\", accuracy)\n","\n","model_stats = pipeline.named_steps['model']\n","\n","# Obtain the number of features used by the model\n","attributes = model_stats.__dict__\n","\n","# Print all attributes\n","for attr, value in attributes.items():\n","    print(attr, \":\", value)\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nQoIB6k0XiRF"},"outputs":[],"source":["import datetime\n","timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","filename = f'G:/MyDrive/Professionele ontwikkeling/Data Science/Explore Data Science Course/Sprint 6_Advanced Classification/Predict/advanced-classification-predict/notebook/dawieloots_predict_gridsearch_{timestamp}.csv'\n","df_gridsearch.to_csv(filename, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_lxVdk5DJ4Ci"},"outputs":[],"source":["\n","print(df_gridsearch)\n","print(grid_search.get_params)"]},{"cell_type":"markdown","metadata":{"id":"K6fbMy5RktY2"},"source":["from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.model_selection import train_test_split\n","\n","from imblearn.pipeline import Pipeline\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif\n","from imblearn.over_sampling import SMOTE\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.datasets import fetch_20newsgroups\n","from sklearn.neighbors import KNeighborsClassifier\n","import numpy as np\n","\n","# Custom transformer to wrap SMOTE\n","\n","class ResampleAndFeatureSelectTransformer(BaseEstimator, TransformerMixin):\n","    def __init__(self, k=1000, score_func=chi2, k_neighbors=5):\n","        self.k = k\n","        self.score_func = score_func\n","        self.k_neighbors = k_neighbors\n","        self.feature_selector = SelectKBest(score_func=self.score_func, k=self.k)\n","        \n","    def fit(self, X, y):\n","        # Print the shape of X_train before preprocessing\n","               \n","        # Select features from the resampled data\n","        self.feature_selector.fit(X, y)\n","        \n","        # Print the shape of X_train after preprocessing\n","        \n","        return self\n","\n","    def transform(self, X):\n","        # Select features from the input data\n","        X_selected = self.feature_selector.transform(X)\n","        return X_selected\n","\n","# Load the 20 Newsgroups dataset\n","data = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n","data_subset = {\n","    'data': data.data[:1000],\n","    'target': data.target[:1000],\n","    'target_names': data.target_names\n","}\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(data_subset['data'], data_subset['target'], test_size=0.2, random_state=42)\n","\n","# Define preprocessing steps\n","preprocessing_steps = [\n","    # Text preprocessing\n","    ('vectorizer', TfidfVectorizer()),  # Convert text data into numerical vectors\n","    ('smote', SMOTE()),\n","    ('resample_and_feature_select', ResampleAndFeatureSelectTransformer(k=1000, score_func=chi2)),  # Resample and select top k features\n","    \n","]\n","\n","# Define the model\n","model = KNeighborsClassifier()\n","\n","# Create the pipeline\n","pipeline = Pipeline(preprocessing_steps + [('model', model)])\n","\n","# Train the pipeline\n","pipeline.fit(X_train, y_train)\n","\n","# Evaluate the pipeline\n","accuracy = pipeline.score(X_test, y_test)\n","print(\"Accuracy:\", accuracy)\n","\n","model = pipeline.named_steps['model']\n","\n","# Obtain the number of features used by the model\n","attributes = model.__dict__\n","\n","# Print all attributes\n","for attr, value in attributes.items():\n","    print(attr, \":\", value)\n","#n_features = model.n_features_\n","#print(\"Number of features used by the model:\", n_features)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AyoIXSk4rU58"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1OxjFEie5eThMMpfvLVib244h1m4KVxzQ","timestamp":1712574548778}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}